#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆçš„åˆæˆæ•°æ®éªŒè¯è„šæœ¬ï¼ˆæ‹’ç»é‡‡æ ·ï¼‰

ä»»åŠ¡: Copa
è®­ç»ƒæ–¹æ³•: mezo
éªŒè¯æ¨¡å‹: gpt-4o
Field to rephrase: premise
ç”Ÿæˆæ—¶é—´: 2025-12-30 18:43:11
"""

from tqdm import tqdm
import os
import json
from openai import OpenAI

# é…ç½®
API_KEY = os.environ.get("OPENAI_API_KEY", "sk-eWSYPo0CvhRYgcJs55B0C3F00aC74f6e95F47c1f4772292c")
API_BASE = os.environ.get("OPENAI_API_BASE", "https://api2.aigcbest.top/v1")

client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE,
    timeout=120
)

# â­ å°è¯•ä»validation_checkpointsåŠ è½½è‡ªåŠ¨ç”Ÿæˆçš„few-shot
# ï¼ˆç”±annotate_samples.pyç”Ÿæˆï¼‰
VALIDATION_FEWSHOT_EXAMPLES = []
try:
    import sys
    from pathlib import Path
    checkpoint_file = Path(__file__).parent.parent / "validation_checkpoints" / "validation_fewshot.json"
    if checkpoint_file.exists():
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            fewshot_data = json.load(f)
            VALIDATION_FEWSHOT_EXAMPLES = fewshot_data.get('examples', [])
        print(f"âœ“ åŠ è½½äº† {len(VALIDATION_FEWSHOT_EXAMPLES)} ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„validation few-shot examples")
except Exception as e:
    print(f"âš ï¸  æœªæ‰¾åˆ°è‡ªåŠ¨ç”Ÿæˆçš„few-shotï¼Œå°†ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„few-shot: {e}")

def generate_validation_prompt(original_premise, original_choice1, original_choice2, original_question, original_label, rephrased_premise):
    """ç”ŸæˆéªŒè¯æç¤ºè¯"""

    # â­ æ„å»ºfew-shotæ–‡æœ¬
    fewshot_text = ""

    # ä¼˜å…ˆä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„few-shotï¼ˆæ¥è‡ªæ ·æœ¬21-40ï¼‰
    if len(VALIDATION_FEWSHOT_EXAMPLES) > 0:
        for i, ex in enumerate(VALIDATION_FEWSHOT_EXAMPLES, 1):
            fewshot_text += f"Example {i}:\n"
            fewshot_text += f"Original premise: {ex.get('original_premise', 'N/A')}\n"
            fewshot_text += f"Rephrased premise: {ex.get('rephrased_premise', 'N/A')}\n"
            # æ·»åŠ å…¶ä»–å­—æ®µ
            for key in ex:
                if not key.startswith('original_') and not key.startswith('rephrased_') and key != 'evaluation':
                    fewshot_text += f"{key}: {ex[key]}\n"
            fewshot_text += f"Evaluation: {ex.get('evaluation', 'same')}\n\n"
    else:
        # å¤‡ç”¨ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æ‰‹åŠ¨æä¾›çš„few-shot
        manual_examples = [{'original_premise': 'My body cast a shadow over the grass.', 'rephrased_premise': 'A shadow from my body fell across the grass.', 'choice1': 'The sun was rising.', 'choice2': 'The grass was cut.', 'question': 'cause', 'label': 0, 'evaluation': 'same'}, {'original_premise': "The woman tolerated her friend's difficult behavior.", 'rephrased_premise': "The woman accepted her friend's challenging conduct.", 'choice1': 'The woman knew her friend was going through a hard time.', 'choice2': 'The woman felt that her friend took advantage of her kindness.', 'question': 'cause', 'label': 0, 'evaluation': 'same'}, {'original_premise': 'The women met for coffee.', 'rephrased_premise': 'The two women decided to gather at a cafÃ©.', 'choice1': 'The cafe reopened in a new location.', 'choice2': 'They wanted to catch up with each other.', 'question': 'cause', 'label': 1, 'evaluation': 'same'}]
        for i, ex in enumerate(manual_examples, 1):
            if isinstance(ex, dict):
                fewshot_text += f"Example {i}:\n"
                for k, v in ex.items():
                    fewshot_text += f"{k}: {v}\n"
                fewshot_text += "\n"

    # â­ åŸå§‹promptæ¨¡æ¿
    prompt_template = """\
Task: Verify if the rephrased premise maintains consistency with the correct answer choice.

{{VALIDATION_FEWSHOT}}

Original premise: "{original_premise}"
Rephrased premise: "{rephrased_premise}"
Choice 1: "{original_choice1}"
Choice 2: "{original_choice2}"
Question: "{original_question}"
Correct answer: "{original_choice1 if original_label == 0 else original_choice2}"

Output [same/not the same]:

"""

    # â­ æ›¿æ¢{{VALIDATION_FEWSHOT}}å ä½ç¬¦
    prompt = prompt_template.replace("{{VALIDATION_FEWSHOT}}", fewshot_text)

    # â­ æ„å»ºå­—æ®µå­—å…¸ç”¨äºformat
    format_dict = {}
    for field in ['premise', 'choice1', 'choice2', 'question', 'label']:
        format_dict[f'original_{field}'] = locals().get(f'original_{field}', '')
    format_dict['rephrased_premise'] = locals().get('rephrased_premise', '')

    # â­ æ›¿æ¢å­—æ®µå€¼
    return prompt.format(**format_dict)
"""

# åŠ è½½åŸå§‹æ•°æ®
original_data = []
with open("/home/ubuntu/LLM-inference/jikai-project/Synthetic_Data_for_ZO/Data/original/Copa/copa_train.jsonl", 'r', encoding='utf-8') as f:
    for line in f:
        original_data.append(json.loads(line.strip()))

# åŠ è½½åˆæˆæ•°æ®
# ğŸ†• ä»æ•°æ®é›†å­ç›®å½•è¯»å–
dataset_dir = os.path.join("/home/ubuntu/LLM-inference/jikai-project/Synthetic_Data_for_ZO/Data_v2/synthetic/_shared/Copa/temp09_topp10_gpt4o", "Copa")
synthetic_data = []
synthetic_file = os.path.join(dataset_dir, "copa_train.jsonl")
with open(synthetic_file, 'r', encoding='utf-8') as f:
    for line in f:
        synthetic_data.append(json.loads(line.strip()))

print(f"åŸå§‹æ•°æ®: {len(original_data)} æ¡")
print(f"åˆæˆæ•°æ®: {len(synthetic_data)} æ¡")

if len(original_data) != len(synthetic_data):
    print("âš  è­¦å‘Š: æ•°æ®é‡ä¸åŒ¹é…!")

# å‡†å¤‡è¾“å‡ºï¼ˆä¸´æ—¶æ–‡ä»¶ï¼‰
temp_output_file = os.path.join(dataset_dir, "copa_train_validated.jsonl")
out_file = open(temp_output_file, "w", encoding='utf-8')

correct_count = 0
total_count = 0

# éªŒè¯æ¯æ¡æ•°æ®
for i in tqdm(range(min(len(original_data), len(synthetic_data)))):
    original = original_data[i]
    synthetic = synthetic_data[i]

    # ğŸ”´ æ’é™¤æ ·æœ¬21-40ï¼ˆç´¢å¼•20-39ï¼‰
    # è¿™äº›æ ·æœ¬ç”¨ä½œjudgerçš„few-shot examplesï¼Œä¸åº”è¢«judgeréªŒè¯ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
    if 20 <= i < 40:
        # ç›´æ¥ä½¿ç”¨åˆæˆæ•°æ®ï¼Œä¸ç»è¿‡judgeréªŒè¯
        out_file.write(json.dumps(synthetic, ensure_ascii=False) + "\n")
        correct_count += 1
        total_count += 1
        out_file.flush()
        continue

    # æ„é€ éªŒè¯æç¤ºè¯
    prompt_args = {}
    for field in ['premise', 'choice1', 'choice2', 'question', 'label']:
        prompt_args[f'original_{field}'] = original[field]
    prompt_args['rephrased_premise'] = synthetic['premise']

    prompt = generate_validation_prompt(**prompt_args)

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful judge."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0
        )

        result = response.choices[0].message.content.strip().lower()

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡éªŒè¯
        if 'not the same' in result or 'not same' in result:
            # éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
            out_file.write(json.dumps(original, ensure_ascii=False) + "\n")
        else:
            # éªŒè¯æˆåŠŸï¼Œä½¿ç”¨åˆæˆæ•°æ®
            out_file.write(json.dumps(synthetic, ensure_ascii=False) + "\n")
            correct_count += 1

        total_count += 1
        out_file.flush()

    except Exception as e:
        print(f"\néªŒè¯ç¬¬ {i} æ¡æ•°æ®æ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶ä½¿ç”¨åŸå§‹æ•°æ®
        out_file.write(json.dumps(original, ensure_ascii=False) + "\n")
        total_count += 1
        out_file.flush()

out_file.close()

accuracy = correct_count / total_count if total_count > 0 else 0
print(f"\néªŒè¯å®Œæˆ!")
print(f"é€šè¿‡ç‡: {correct_count}/{total_count} = {accuracy:.2%}")
print(f"ä¸´æ—¶è¾“å‡ºæ–‡ä»¶: {temp_output_file}")

# ğŸ†• æœ€ç»ˆåŒ–æ•°æ®é›†ï¼šé‡å‘½åvalidatedæ–‡ä»¶ + å¤åˆ¶validation/test
print("\næœ€ç»ˆåŒ–æ•°æ®é›†...")
import shutil

# 1. å°†validatedæ–‡ä»¶é‡å‘½åä¸ºæ­£å¼çš„trainæ–‡ä»¶
final_train_file = os.path.join(dataset_dir, "copa_train.jsonl")
if os.path.exists(final_train_file):
    os.remove(final_train_file)  # åˆ é™¤åŸå§‹çš„æœªéªŒè¯æ–‡ä»¶
shutil.move(temp_output_file, final_train_file)
print(f"âœ“ è®­ç»ƒé›†: {final_train_file}")

# 2. å¤åˆ¶validationå’Œtestæ–‡ä»¶fromåŸå§‹æ•°æ®é›†
original_dir = "/home/ubuntu/LLM-inference/jikai-project/Synthetic_Data_for_ZO/Data/original/Copa"
files_config = {'train': 'copa_train.jsonl', 'validation': 'copa_validation.jsonl', 'test': 'copa_test.jsonl'}

# å¤åˆ¶validationæ–‡ä»¶
if 'validation' in files_config:
    val_file = files_config['validation']
    src_val = os.path.join(original_dir, val_file)
    dst_val = os.path.join(dataset_dir, val_file)
    if os.path.exists(src_val):
        shutil.copy2(src_val, dst_val)
        print(f"âœ“ éªŒè¯é›†: {dst_val}")
    else:
        print(f"âš   è­¦å‘Š: éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨: {src_val}")

# å¤åˆ¶testæ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
if 'test' in files_config:
    test_file = files_config['test']
    src_test = os.path.join(original_dir, test_file)
    dst_test = os.path.join(dataset_dir, test_file)
    if os.path.exists(src_test):
        shutil.copy2(src_test, dst_test)
        print(f"âœ“ æµ‹è¯•é›†: {dst_test}")

print(f"\nâœ… æ•°æ®é›†å·²å®Œæˆï¼å¯ç”¨äºMeZOè®­ç»ƒï¼š")
print(f"   python PromptZO/MeZO/large_models/run.py --task {dataset_dir}")
