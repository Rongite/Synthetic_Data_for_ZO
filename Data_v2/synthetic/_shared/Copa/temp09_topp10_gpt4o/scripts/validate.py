#!/usr/bin/env python3
"""
Automatically generated synthetic data validation scriptÔºàRejection samplingÔºâ

Task: Copa
Training Method: mezo
ValidationModel: gpt-4o
Field to rephrase: premise
Generation Time: 2025-12-30 18:43:11
"""

from tqdm import tqdm
import os
import json
from openai import OpenAI

# Configuration
API_KEY = os.environ.get("OPENAI_API_KEY", "sk-eWSYPo0CvhRYgcJs55B0C3F00aC74f6e95F47c1f4772292c")
API_BASE = os.environ.get("OPENAI_API_BASE", "https://api2.aigcbest.top/v1")

client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE,
    timeout=120
)

# ‚≠ê Try to load automatically generated few-shot from validation_checkpoints
# ÔºàGenerated by annotate_samples.pyÔºâ
VALIDATION_FEWSHOT_EXAMPLES = []
try:
    import sys
    from pathlib import Path
    checkpoint_file = Path(__file__).parent.parent / "validation_checkpoints" / "validation_fewshot.json"
    if checkpoint_file.exists():
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            fewshot_data = json.load(f)
            VALIDATION_FEWSHOT_EXAMPLES = fewshot_data.get('examples', [])
        print(f"‚úì Loaded {len(VALIDATION_FEWSHOT_EXAMPLES)} automatically generated validation few-shot examples")
except Exception as e:
    print(f"‚ö†Ô∏è  Automatically generated few-shot not found, will use few-shot from configuration file: {e}")

def generate_validation_prompt(original_premise, original_choice1, original_choice2, original_question, original_label, rephrased_premise):
    """Generate validation prompt"""

    # ‚≠ê Build few-shot text
    fewshot_text = ""

    # Prefer to use automatically generated few-shot (from samples 21-40)
    if len(VALIDATION_FEWSHOT_EXAMPLES) > 0:
        for i, ex in enumerate(VALIDATION_FEWSHOT_EXAMPLES, 1):
            fewshot_text += f"Example {i}:\n"
            fewshot_text += f"Original premise: {ex.get('original_premise', 'N/A')}\n"
            fewshot_text += f"Rephrased premise: {ex.get('rephrased_premise', 'N/A')}\n"
            # add other fields
            for key in ex:
                if not key.startswith('original_') and not key.startswith('rephrased_') and key != 'evaluation':
                    fewshot_text += f"{key}: {ex[key]}\n"
            fewshot_text += f"Evaluation: {ex.get('evaluation', 'same')}\n\n"
    else:
        # Fallback: use manually provided few-shot from configuration file
        manual_examples = [{'original_premise': 'My body cast a shadow over the grass.', 'rephrased_premise': 'A shadow from my body fell across the grass.', 'choice1': 'The sun was rising.', 'choice2': 'The grass was cut.', 'question': 'cause', 'label': 0, 'evaluation': 'same'}, {'original_premise': "The woman tolerated her friend's difficult behavior.", 'rephrased_premise': "The woman accepted her friend's challenging conduct.", 'choice1': 'The woman knew her friend was going through a hard time.', 'choice2': 'The woman felt that her friend took advantage of her kindness.', 'question': 'cause', 'label': 0, 'evaluation': 'same'}, {'original_premise': 'The women met for coffee.', 'rephrased_premise': 'The two women decided to gather at a caf√©.', 'choice1': 'The cafe reopened in a new location.', 'choice2': 'They wanted to catch up with each other.', 'question': 'cause', 'label': 1, 'evaluation': 'same'}]
        for i, ex in enumerate(manual_examples, 1):
            if isinstance(ex, dict):
                fewshot_text += f"Example {i}:\n"
                for k, v in ex.items():
                    fewshot_text += f"{k}: {v}\n"
                fewshot_text += "\n"

    # ‚≠ê Original prompt template
    prompt_template = """\
Task: Verify if the rephrased premise maintains consistency with the correct answer choice.

{{VALIDATION_FEWSHOT}}

Original premise: "{original_premise}"
Rephrased premise: "{rephrased_premise}"
Choice 1: "{original_choice1}"
Choice 2: "{original_choice2}"
Question: "{original_question}"
Correct answer: "{original_choice1 if original_label == 0 else original_choice2}"

Output [same/not the same]:

"""

    # ‚≠ê Replace {{VALIDATION_FEWSHOT}} placeholder
    prompt = prompt_template.replace("{{VALIDATION_FEWSHOT}}", fewshot_text)

    # ‚≠ê build field dictionary for formatting
    format_dict = {}
    for field in ['premise', 'choice1', 'choice2', 'question', 'label']:
        format_dict[f'original_{field}'] = locals().get(f'original_{field}', '')
    format_dict['rephrased_premise'] = locals().get('rephrased_premise', '')

    # ‚≠ê Replace field values
    return prompt.format(**format_dict)
"""

# Loaded original data
original_data = []
with open("/home/ubuntu/LLM-inference/jikai-project/Synthetic_Data_for_ZO/Data/original/Copa/copa_train.jsonl", 'r', encoding='utf-8') as f:
    for line in f:
        original_data.append(json.loads(line.strip()))

# Loaded synthetic data
# üÜï  read from dataset subdirectory
dataset_dir = os.path.join("/home/ubuntu/LLM-inference/jikai-project/Synthetic_Data_for_ZO/Data_v2/synthetic/_shared/Copa/temp09_topp10_gpt4o", "Copa")
synthetic_data = []
synthetic_file = os.path.join(dataset_dir, "copa_train.jsonl")
with open(synthetic_file, 'r', encoding='utf-8') as f:
    for line in f:
        synthetic_data.append(json.loads(line.strip()))

print(f"Original data: {len(original_data)} samples")
print(f"Synthetic data: {len(synthetic_data)} samples")

if len(original_data) != len(synthetic_data):
    print("‚ö† warning: Data count mismatch!")

# Prepare output (temporary file)
temp_output_file = os.path.join(dataset_dir, "copa_train_validated.jsonl")
out_file = open(temp_output_file, "w", encoding='utf-8')

correct_count = 0
total_count = 0

# Validate each data sample
for i in tqdm(range(min(len(original_data), len(synthetic_data)))):
    original = original_data[i]
    synthetic = synthetic_data[i]

    # üî¥ Exclude samples 21-40 (index 20-39)
    # These samples are used as judger few-shot examples, should not be validated by judger (avoid data leakage)
    if 20 <= i < 40:
        # Directly use synthetic data without judger validation
        out_file.write(json.dumps(synthetic, ensure_ascii=False) + "\n")
        correct_count += 1
        total_count += 1
        out_file.flush()
        continue

    # Construct validation prompt
    prompt_args = {}
    for field in ['premise', 'choice1', 'choice2', 'question', 'label']:
        prompt_args[f'original_{field}'] = original[field]
    prompt_args['rephrased_premise'] = synthetic['premise']

    prompt = generate_validation_prompt(**prompt_args)

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You are a helpful judge."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0
        )

        result = response.choices[0].message.content.strip().lower()

        # Determine if validation passes
        if 'not the same' in result or 'not same' in result:
            # Validation failed, use original data
            out_file.write(json.dumps(original, ensure_ascii=False) + "\n")
        else:
            # Validation successful, use synthetic data
            out_file.write(json.dumps(synthetic, ensure_ascii=False) + "\n")
            correct_count += 1

        total_count += 1
        out_file.flush()

    except Exception as e:
        print(f"\nError validating sample {i}  samplesDataError when: {e}")
        # Use original data on error
        out_file.write(json.dumps(original, ensure_ascii=False) + "\n")
        total_count += 1
        out_file.flush()

out_file.close()

accuracy = correct_count / total_count if total_count > 0 else 0
print(f"\nValidation Complete!")
print(f"Pass rate: {correct_count}/{total_count} = {accuracy:.2%}")
print(f"temporary output file: {temp_output_file}")

# üÜï Finalize dataset: Rename validated file + Copy validation/test
print("\nFinalizing dataset...")
import shutil

# 1.  Rename validated file as official train file
final_train_file = os.path.join(dataset_dir, "copa_train.jsonl")
if os.path.exists(final_train_file):
    os.remove(final_train_file)  # Delete original unvalidated file
shutil.move(temp_output_file, final_train_file)
print(f"‚úì Training set: {final_train_file}")

# 2. Copy validation and test files from original dataset
original_dir = "/home/ubuntu/LLM-inference/jikai-project/Synthetic_Data_for_ZO/Data/original/Copa"
files_config = {'train': 'copa_train.jsonl', 'validation': 'copa_validation.jsonl', 'test': 'copa_test.jsonl'}

# Copy validation file
if 'validation' in files_config:
    val_file = files_config['validation']
    src_val = os.path.join(original_dir, val_file)
    dst_val = os.path.join(dataset_dir, val_file)
    if os.path.exists(src_val):
        shutil.copy2(src_val, dst_val)
        print(f"‚úì Validation set: {dst_val}")
    else:
        print(f"‚ö†  warning: Validation set file does not exist: {src_val}")

# Copy test file (if exists)
if 'test' in files_config:
    test_file = files_config['test']
    src_test = os.path.join(original_dir, test_file)
    dst_test = os.path.join(dataset_dir, test_file)
    if os.path.exists(src_test):
        shutil.copy2(src_test, dst_test)
        print(f"‚úì Test set: {dst_test}")

print(f"\n‚úÖ Dataset is Complete! Can be used for MeZO training:")
print(f"   python PromptZO/MeZO/large_models/run.py --task {dataset_dir}")
