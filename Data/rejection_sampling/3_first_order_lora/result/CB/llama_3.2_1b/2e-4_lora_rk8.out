firstorder-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB-lora-16-2e-4-0-lora-8
STEPS: 20000
BS: 16
LR: 2e-4
EPS: 1e-3
SEED: 0
MODE: lora
LoRA RANK: 8
Extra args: --lora --lora_rank 8 
OurArguments(
C4_grad_addr=None,
C4_grad_mask=False,
GraSP_mask=False,
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
dynamic_mask=False,
dynamic_mask_step=100,
eos_token=<EOS_TOKEN>,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=500,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
grad_mask=False,
grad_save_path=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
icl_sfc=False,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0002,
length_column_name=length,
linear_probing=False,
load_best_model_at_end=True,
load_bfloat16=False,
load_float16=False,
load_int8=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/jlong1/Downloads/models/synthetic_data/fo_lora/synthetic/CB-Llama-3.2-1B-firstorder-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB-lora-16-2e-4-0-lora-8/runs/Apr08_15-11-21_siai-3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lora=True,
lora_alpha=16,
lora_rank=8,
lp_early_stopping=False,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
mask_path=None,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=50,
max_steps=20000,
memory_limit_scenario=False,
metric_for_best_model=loss,
model_name=meta-llama/Llama-3.2-1B,
model_weight_path=None,
mp_parameters=,
n_tokens=2,
neftune_noise_alpha=None,
no_auto_device=False,
no_cuda=False,
no_eval=False,
no_flash_attn_2=False,
no_reparam=True,
non_diff=False,
num_beams=1,
num_dev=100,
num_eval=1000,
num_prefix=5,
num_train=1000,
num_train_epochs=3.0,
num_train_sets=None,
only_train_option=True,
optim=sgd,
optim_args=None,
optim_target_modules=None,
other_task_mask=False,
outlier=False,
outlier_percentage=0.005,
output_dir=/home/jlong1/Downloads/models/synthetic_data/fo_lora/synthetic/CB-Llama-3.2-1B-firstorder-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB-lora-16-2e-4-0-lora-8,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
prefix_init_by_real_act=True,
prefix_layer_id=None,
prefix_tuning=False,
prefix_tuning_one_layer=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
random_subset_weights=False,
ray_scope=last,
record_time=True,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
restore_callback_states_from_checkpoint=False,
result_file=None,
resume_from_checkpoint=None,
run_name=/home/jlong1/Downloads/models/synthetic_data/fo_lora/synthetic/CB-Llama-3.2-1B-firstorder-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB-lora-16-2e-4-0-lora-8,
samples=1,
sampling=True,
save_grad=False,
save_model=False,
save_model_addr=None,
save_on_each_node=False,
save_on_interrupt=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
sfc=False,
skip_memory_metrics=True,
smaller_weight_mask=False,
split_batches=None,
squeezellm_ckpt=None,
squeezellm_wbits=4,
tag=firstorder-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB-lora-16-2e-4-0-lora-8,
task_name=/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB,
temperature=1.0,
tf32=None,
top_k=None,
top_p=0.95,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_as_classification=True,
train_set_seed=0,
trainer=regular,
use_adam=False,
use_cpu=False,
use_full_zo_update=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_momentum=False,
use_mps_device=False,
use_sgd=False,
use_squeezellm=False,
use_squeezellm_peft=False,
verbose=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
zo_eps=0.001,
)
Detected local path: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB
Loading CB dataset from local JSONL files: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB
Loaded 250 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB/cb_train.jsonl
Loaded 56 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/rejection_sampling/0_data/CB/cb_validation.jsonl
Number of training samples loaded: 250
Number of validation samples loaded: 56
Number of training samples after label processing: 250
Number of validation samples after label processing: 56
Number of training samples after building: 250
Number of validation samples after building: 56
Training set sample count: 250
Validation set sample count: 56
The training set is completely sequential
Loading model...
Done with 2.48s
Inject lora to layer 0
Inject lora to layer 1
Inject lora to layer 2
Inject lora to layer 3
Inject lora to layer 4
Inject lora to layer 5
Inject lora to layer 6
Inject lora to layer 7
Inject lora to layer 8
Inject lora to layer 9
Inject lora to layer 10
Inject lora to layer 11
Inject lora to layer 12
Inject lora to layer 13
Inject lora to layer 14
Inject lora to layer 15
Tokenizing training samples...
Done with 0.26s
[2025-04-08 15:11:31,965] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.0002
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)
