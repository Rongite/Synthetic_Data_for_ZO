mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0
STEPS: 20000
BS: 16
LR: 1e-6
EPS: 1e-3
SEED: 0
MODE: ft
Extra args:  
mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0
STEPS: 20000
BS: 16
LR: 1e-6
EPS: 1e-3
SEED: 0
MODE: ft
Extra args:  
OurArguments(
C4_grad_addr=None,
C4_grad_mask=False,
GraSP_mask=False,
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
dynamic_mask=False,
dynamic_mask_step=100,
eos_token=<EOS_TOKEN>,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=500,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
grad_mask=False,
grad_save_path=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
icl_sfc=False,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
linear_probing=False,
load_best_model_at_end=True,
load_bfloat16=False,
load_float16=False,
load_int8=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/grand/sbi-fair/jikaiLoong/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0/runs/Feb20_07-53-52_siai-3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lora=False,
lora_alpha=16,
lora_rank=16,
lp_early_stopping=False,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
mask_path=None,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=50,
max_steps=20000,
memory_limit_scenario=False,
metric_for_best_model=loss,
model_name=meta-llama/Llama-3.2-1B,
model_weight_path=None,
mp_parameters=,
n_tokens=2,
neftune_noise_alpha=None,
no_auto_device=False,
no_cuda=False,
no_eval=False,
no_flash_attn_2=False,
no_reparam=True,
non_diff=False,
num_beams=1,
num_dev=100,
num_eval=1000,
num_prefix=5,
num_train=1000,
num_train_epochs=3.0,
num_train_sets=None,
only_train_option=True,
optim=sgd,
optim_args=None,
optim_target_modules=None,
other_task_mask=False,
outlier=False,
outlier_percentage=0.005,
output_dir=/grand/sbi-fair/jikaiLoong/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
prefix_init_by_real_act=True,
prefix_layer_id=None,
prefix_tuning=False,
prefix_tuning_one_layer=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
random_subset_weights=False,
ray_scope=last,
record_time=True,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
restore_callback_states_from_checkpoint=False,
result_file=None,
resume_from_checkpoint=None,
run_name=/grand/sbi-fair/jikaiLoong/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
samples=1,
sampling=True,
save_grad=False,
save_model=False,
save_model_addr=None,
save_on_each_node=False,
save_on_interrupt=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
sfc=False,
skip_memory_metrics=True,
smaller_weight_mask=False,
split_batches=None,
squeezellm_ckpt=None,
squeezellm_wbits=4,
tag=mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
task_name=/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB,
temperature=1.0,
tf32=None,
top_k=None,
top_p=0.95,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_as_classification=True,
train_set_seed=0,
trainer=zo,
use_adam=False,
use_cpu=False,
use_full_zo_update=True,
use_ipex=False,
use_legacy_prediction_loop=False,
use_momentum=False,
use_mps_device=False,
use_sgd=False,
use_squeezellm=False,
use_squeezellm_peft=False,
verbose=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
zo_eps=0.001,
)
Detected local path: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loading CB dataset from local JSONL files: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loaded 250 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_train.jsonl
Loaded 56 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_validation.jsonl
Number of training samples loaded: 250
Number of validation samples loaded: 56
Number of training samples after label processing: 250
Number of validation samples after label processing: 56
Number of training samples after building: 250
Number of validation samples after building: 56
Training set sample count: 250
Validation set sample count: 56
The training set is completely sequential
Loading model...
Done with 2.85s
Tokenizing training samples...
Done with 0.26s
mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0
STEPS: 20000
BS: 16
LR: 1e-6
EPS: 1e-3
SEED: 0
MODE: ft
Extra args:  
OurArguments(
C4_grad_addr=None,
C4_grad_mask=False,
GraSP_mask=False,
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
dynamic_mask=False,
dynamic_mask_step=100,
eos_token=<EOS_TOKEN>,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=500,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
grad_mask=False,
grad_save_path=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
icl_sfc=False,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
linear_probing=False,
load_best_model_at_end=True,
load_bfloat16=False,
load_float16=False,
load_int8=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0/runs/Feb20_07-55-11_siai-3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lora=False,
lora_alpha=16,
lora_rank=16,
lp_early_stopping=False,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
mask_path=None,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=50,
max_steps=20000,
memory_limit_scenario=False,
metric_for_best_model=loss,
model_name=meta-llama/Llama-3.2-1B,
model_weight_path=None,
mp_parameters=,
n_tokens=2,
neftune_noise_alpha=None,
no_auto_device=False,
no_cuda=False,
no_eval=False,
no_flash_attn_2=False,
no_reparam=True,
non_diff=False,
num_beams=1,
num_dev=100,
num_eval=1000,
num_prefix=5,
num_train=1000,
num_train_epochs=3.0,
num_train_sets=None,
only_train_option=True,
optim=sgd,
optim_args=None,
optim_target_modules=None,
other_task_mask=False,
outlier=False,
outlier_percentage=0.005,
output_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
prefix_init_by_real_act=True,
prefix_layer_id=None,
prefix_tuning=False,
prefix_tuning_one_layer=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
random_subset_weights=False,
ray_scope=last,
record_time=True,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
restore_callback_states_from_checkpoint=False,
result_file=None,
resume_from_checkpoint=None,
run_name=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
samples=1,
sampling=True,
save_grad=False,
save_model=False,
save_model_addr=None,
save_on_each_node=False,
save_on_interrupt=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
sfc=False,
skip_memory_metrics=True,
smaller_weight_mask=False,
split_batches=None,
squeezellm_ckpt=None,
squeezellm_wbits=4,
tag=mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
task_name=/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB,
temperature=1.0,
tf32=None,
top_k=None,
top_p=0.95,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_as_classification=True,
train_set_seed=0,
trainer=zo,
use_adam=False,
use_cpu=False,
use_full_zo_update=True,
use_ipex=False,
use_legacy_prediction_loop=False,
use_momentum=False,
use_mps_device=False,
use_sgd=False,
use_squeezellm=False,
use_squeezellm_peft=False,
verbose=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
zo_eps=0.001,
)
Detected local path: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loading CB dataset from local JSONL files: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loaded 250 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_train.jsonl
Loaded 56 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_validation.jsonl
Number of training samples loaded: 250
Number of validation samples loaded: 56
Number of training samples after label processing: 250
Number of validation samples after label processing: 56
Number of training samples after building: 250
Number of validation samples after building: 56
Training set sample count: 250
Validation set sample count: 56
The training set is completely sequential
Loading model...
Done with 1.78s
Tokenizing training samples...
Done with 0.36s
[2025-02-20 07:55:13,543] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 1e-06
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)
mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0
STEPS: 20000
BS: 16
LR: 1e-6
EPS: 1e-3
SEED: 0
MODE: ft
Extra args:  
OurArguments(
C4_grad_addr=None,
C4_grad_mask=False,
GraSP_mask=False,
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
dynamic_mask=False,
dynamic_mask_step=100,
eos_token=<EOS_TOKEN>,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=500,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
grad_mask=False,
grad_save_path=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
icl_sfc=False,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
linear_probing=False,
load_best_model_at_end=True,
load_bfloat16=False,
load_float16=False,
load_int8=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0/runs/Feb20_07-59-35_siai-3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lora=False,
lora_alpha=16,
lora_rank=16,
lp_early_stopping=False,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
mask_path=None,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=50,
max_steps=20000,
memory_limit_scenario=False,
metric_for_best_model=loss,
model_name=meta-llama/Llama-3.2-1B,
model_weight_path=None,
mp_parameters=,
n_tokens=2,
neftune_noise_alpha=None,
no_auto_device=False,
no_cuda=False,
no_eval=False,
no_flash_attn_2=False,
no_reparam=True,
non_diff=False,
num_beams=1,
num_dev=100,
num_eval=1000,
num_prefix=5,
num_train=1000,
num_train_epochs=3.0,
num_train_sets=None,
only_train_option=True,
optim=sgd,
optim_args=None,
optim_target_modules=None,
other_task_mask=False,
outlier=False,
outlier_percentage=0.005,
output_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
prefix_init_by_real_act=True,
prefix_layer_id=None,
prefix_tuning=False,
prefix_tuning_one_layer=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
random_subset_weights=False,
ray_scope=last,
record_time=True,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
restore_callback_states_from_checkpoint=False,
result_file=None,
resume_from_checkpoint=None,
run_name=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
samples=1,
sampling=True,
save_grad=False,
save_model=False,
save_model_addr=None,
save_on_each_node=False,
save_on_interrupt=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
sfc=False,
skip_memory_metrics=True,
smaller_weight_mask=False,
split_batches=None,
squeezellm_ckpt=None,
squeezellm_wbits=4,
tag=mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
task_name=/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB,
temperature=1.0,
tf32=None,
top_k=None,
top_p=0.95,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_as_classification=True,
train_set_seed=0,
trainer=zo,
use_adam=False,
use_cpu=False,
use_full_zo_update=True,
use_ipex=False,
use_legacy_prediction_loop=False,
use_momentum=False,
use_mps_device=False,
use_sgd=False,
use_squeezellm=False,
use_squeezellm_peft=False,
verbose=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
zo_eps=0.001,
)
Detected local path: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loading CB dataset from local JSONL files: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loaded 250 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_train.jsonl
Loaded 56 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_validation.jsonl
Number of training samples loaded: 250
Number of validation samples loaded: 56
Number of training samples after label processing: 250
Number of validation samples after label processing: 56
Number of training samples after building: 250
Number of validation samples after building: 56
Training set sample count: 250
Validation set sample count: 56
The training set is completely sequential
Loading model...
Done with 3.09s
Tokenizing training samples...
Done with 0.26s
[2025-02-20 07:59:39,312] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 1e-06
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)
mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0
STEPS: 20000
BS: 16
LR: 1e-6
EPS: 1e-3
SEED: 0
MODE: ft
Extra args:  
OurArguments(
C4_grad_addr=None,
C4_grad_mask=False,
GraSP_mask=False,
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
dynamic_mask=False,
dynamic_mask_step=100,
eos_token=<EOS_TOKEN>,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=500,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
grad_mask=False,
grad_save_path=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
icl_sfc=False,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
linear_probing=False,
load_best_model_at_end=True,
load_bfloat16=False,
load_float16=False,
load_int8=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0/runs/Feb20_08-14-26_siai-3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lora=False,
lora_alpha=16,
lora_rank=16,
lp_early_stopping=False,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
mask_path=None,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=50,
max_steps=20000,
memory_limit_scenario=False,
metric_for_best_model=loss,
model_name=meta-llama/Llama-3.2-1B,
model_weight_path=None,
mp_parameters=,
n_tokens=2,
neftune_noise_alpha=None,
no_auto_device=False,
no_cuda=False,
no_eval=False,
no_flash_attn_2=False,
no_reparam=True,
non_diff=False,
num_beams=1,
num_dev=100,
num_eval=1000,
num_prefix=5,
num_train=1000,
num_train_epochs=3.0,
num_train_sets=None,
only_train_option=True,
optim=sgd,
optim_args=None,
optim_target_modules=None,
other_task_mask=False,
outlier=False,
outlier_percentage=0.005,
output_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
prefix_init_by_real_act=True,
prefix_layer_id=None,
prefix_tuning=False,
prefix_tuning_one_layer=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
random_subset_weights=False,
ray_scope=last,
record_time=True,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
restore_callback_states_from_checkpoint=False,
result_file=None,
resume_from_checkpoint=None,
run_name=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
samples=1,
sampling=True,
save_grad=False,
save_model=False,
save_model_addr=None,
save_on_each_node=False,
save_on_interrupt=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
sfc=False,
skip_memory_metrics=True,
smaller_weight_mask=False,
split_batches=None,
squeezellm_ckpt=None,
squeezellm_wbits=4,
tag=mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
task_name=/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB,
temperature=1.0,
tf32=None,
top_k=None,
top_p=0.95,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_as_classification=True,
train_set_seed=0,
trainer=zo,
use_adam=False,
use_cpu=False,
use_full_zo_update=True,
use_ipex=False,
use_legacy_prediction_loop=False,
use_momentum=False,
use_mps_device=False,
use_sgd=False,
use_squeezellm=False,
use_squeezellm_peft=False,
verbose=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
zo_eps=0.001,
)
Detected local path: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loading CB dataset from local JSONL files: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loaded 250 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_train.jsonl
Loaded 56 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_validation.jsonl
Number of training samples loaded: 250
Number of validation samples loaded: 56
Number of training samples after label processing: 250
Number of validation samples after label processing: 56
Number of training samples after building: 250
Number of validation samples after building: 56
Training set sample count: 250
Validation set sample count: 56
The training set is completely sequential
Loading model...
Done with 2.47s
Tokenizing training samples...
Done with 0.26s
[2025-02-20 08:14:29,726] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 1e-06
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)
mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0
STEPS: 20000
BS: 16
LR: 1e-6
EPS: 1e-3
SEED: 0
MODE: ft
Extra args:  
OurArguments(
C4_grad_addr=None,
C4_grad_mask=False,
GraSP_mask=False,
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
dynamic_mask=False,
dynamic_mask_step=100,
eos_token=<EOS_TOKEN>,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=500,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
grad_mask=False,
grad_save_path=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
icl_sfc=False,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
linear_probing=False,
load_best_model_at_end=True,
load_bfloat16=False,
load_float16=False,
load_int8=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0/runs/Feb20_10-35-58_siai-3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lora=False,
lora_alpha=16,
lora_rank=16,
lp_early_stopping=False,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
mask_path=None,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=50,
max_steps=20000,
memory_limit_scenario=False,
metric_for_best_model=loss,
model_name=meta-llama/Llama-3.2-1B,
model_weight_path=None,
mp_parameters=,
n_tokens=2,
neftune_noise_alpha=None,
no_auto_device=False,
no_cuda=False,
no_eval=False,
no_flash_attn_2=False,
no_reparam=True,
non_diff=False,
num_beams=1,
num_dev=100,
num_eval=1000,
num_prefix=5,
num_train=1000,
num_train_epochs=3.0,
num_train_sets=None,
only_train_option=True,
optim=sgd,
optim_args=None,
optim_target_modules=None,
other_task_mask=False,
outlier=False,
outlier_percentage=0.005,
output_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
prefix_init_by_real_act=True,
prefix_layer_id=None,
prefix_tuning=False,
prefix_tuning_one_layer=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
random_subset_weights=False,
ray_scope=last,
record_time=True,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
restore_callback_states_from_checkpoint=False,
result_file=None,
resume_from_checkpoint=None,
run_name=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
samples=1,
sampling=True,
save_grad=False,
save_model=False,
save_model_addr=None,
save_on_each_node=False,
save_on_interrupt=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
sfc=False,
skip_memory_metrics=True,
smaller_weight_mask=False,
split_batches=None,
squeezellm_ckpt=None,
squeezellm_wbits=4,
tag=mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
task_name=/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB,
temperature=1.0,
tf32=None,
top_k=None,
top_p=0.95,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_as_classification=True,
train_set_seed=0,
trainer=zo,
use_adam=False,
use_cpu=False,
use_full_zo_update=True,
use_ipex=False,
use_legacy_prediction_loop=False,
use_momentum=False,
use_mps_device=False,
use_sgd=False,
use_squeezellm=False,
use_squeezellm_peft=False,
verbose=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
zo_eps=0.001,
)
Detected local path: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loading CB dataset from local JSONL files: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loaded 250 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_train.jsonl
Loaded 56 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_validation.jsonl
Number of training samples loaded: 250
Number of validation samples loaded: 56
Number of training samples after label processing: 250
Number of validation samples after label processing: 56
Number of training samples after building: 250
Number of validation samples after building: 56
Training set sample count: 250
Validation set sample count: 56
The training set is completely sequential
Loading model...
Done with 2.02s
Tokenizing training samples...
Done with 0.26s
[2025-02-20 10:36:00,593] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 1e-06
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)
mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0
STEPS: 20000
BS: 16
LR: 1e-6
EPS: 1e-3
SEED: 0
MODE: ft
Extra args:  
OurArguments(
C4_grad_addr=None,
C4_grad_mask=False,
GraSP_mask=False,
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
dynamic_mask=False,
dynamic_mask_step=100,
eos_token=<EOS_TOKEN>,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=500,
eval_strategy=steps,
eval_use_gather_object=False,
evaluation_strategy=steps,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
grad_mask=False,
grad_save_path=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
icl_sfc=False,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-06,
length_column_name=length,
linear_probing=False,
load_best_model_at_end=True,
load_bfloat16=False,
load_float16=False,
load_int8=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0/runs/Feb20_14-41-50_siai-3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lora=False,
lora_alpha=16,
lora_rank=16,
lp_early_stopping=False,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
mask_path=None,
max_grad_norm=1.0,
max_length=2048,
max_new_tokens=50,
max_steps=20000,
memory_limit_scenario=False,
metric_for_best_model=loss,
model_name=meta-llama/Llama-3.2-1B,
model_weight_path=None,
mp_parameters=,
n_tokens=2,
neftune_noise_alpha=None,
no_auto_device=False,
no_cuda=False,
no_eval=False,
no_flash_attn_2=False,
no_reparam=True,
non_diff=False,
num_beams=1,
num_dev=100,
num_eval=1000,
num_prefix=5,
num_train=1000,
num_train_epochs=3.0,
num_train_sets=None,
only_train_option=True,
optim=sgd,
optim_args=None,
optim_target_modules=None,
other_task_mask=False,
outlier=False,
outlier_percentage=0.005,
output_dir=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
prefix_init_by_real_act=True,
prefix_layer_id=None,
prefix_tuning=False,
prefix_tuning_one_layer=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
random_subset_weights=False,
ray_scope=last,
record_time=True,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
restore_callback_states_from_checkpoint=False,
result_file=None,
resume_from_checkpoint=None,
run_name=/home/jlong1/Downloads/models/synthetic_data/zo/original/CB-Llama-3.2-1B-mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
samples=1,
sampling=True,
save_grad=False,
save_model=False,
save_model_addr=None,
save_on_each_node=False,
save_on_interrupt=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=1,
seed=42,
sfc=False,
skip_memory_metrics=True,
smaller_weight_mask=False,
split_batches=None,
squeezellm_ckpt=None,
squeezellm_wbits=4,
tag=mezo-meta-llama/Llama-3.2-1B-/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB-ft-16-1e-6-1e-3-0,
task_name=/home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB,
temperature=1.0,
tf32=None,
top_k=None,
top_p=0.95,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_as_classification=True,
train_set_seed=0,
trainer=zo,
use_adam=False,
use_cpu=False,
use_full_zo_update=True,
use_ipex=False,
use_legacy_prediction_loop=False,
use_momentum=False,
use_mps_device=False,
use_sgd=False,
use_squeezellm=False,
use_squeezellm_peft=False,
verbose=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
zo_eps=0.001,
)
Detected local path: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loading CB dataset from local JSONL files: /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB
Loaded 250 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_train.jsonl
Loaded 56 samples from /home/jlong1/Downloads/Synthetic_Data_for_ZO/Data/original/CB/cb_validation.jsonl
Number of training samples loaded: 250
Number of validation samples loaded: 56
Number of training samples after label processing: 250
Number of validation samples after label processing: 56
Number of training samples after building: 250
Number of validation samples after building: 56
Training set sample count: 250
Validation set sample count: 56
The training set is completely sequential
Loading model...
Done with 2.59s
Tokenizing training samples...
Done with 0.26s
[2025-02-20 14:41:53,864] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 1e-06
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)
{'loss': 0.793, 'learning_rate': 1e-06, 'epoch': 50.0}
{'eval_dev_loss': 0.9881276488304138, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 4.2206, 'eval_dev_samples_per_second': 23.693, 'eval_dev_steps_per_second': 3.08, 'epoch': 50.0}
{'eval_test_loss': 0.9037335515022278, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.4353, 'eval_test_samples_per_second': 39.016, 'eval_test_steps_per_second': 4.877, 'epoch': 50.0}
{'loss': 0.6754, 'learning_rate': 1e-06, 'epoch': 100.0}
{'eval_dev_loss': 0.860709547996521, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 4.2283, 'eval_dev_samples_per_second': 23.65, 'eval_dev_steps_per_second': 3.075, 'epoch': 100.0}
{'eval_test_loss': 0.8470555543899536, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.4326, 'eval_test_samples_per_second': 39.089, 'eval_test_steps_per_second': 4.886, 'epoch': 100.0}
{'loss': 0.6121, 'learning_rate': 1e-06, 'epoch': 150.0}
{'eval_dev_loss': 0.8605390191078186, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 4.2638, 'eval_dev_samples_per_second': 23.453, 'eval_dev_steps_per_second': 3.049, 'epoch': 150.0}
{'eval_test_loss': 0.8968300223350525, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.5455, 'eval_test_samples_per_second': 36.235, 'eval_test_steps_per_second': 4.529, 'epoch': 150.0}
{'loss': 0.5865, 'learning_rate': 1e-06, 'epoch': 200.0}
{'eval_dev_loss': 0.7938230037689209, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1178, 'eval_dev_samples_per_second': 47.22, 'eval_dev_steps_per_second': 6.139, 'epoch': 200.0}
{'eval_test_loss': 0.8368483781814575, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2023, 'eval_test_samples_per_second': 46.577, 'eval_test_steps_per_second': 5.822, 'epoch': 200.0}
{'loss': 0.5775, 'learning_rate': 1e-06, 'epoch': 250.0}
{'eval_dev_loss': 1.039862036705017, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1148, 'eval_dev_samples_per_second': 47.286, 'eval_dev_steps_per_second': 6.147, 'epoch': 250.0}
{'eval_test_loss': 0.9448947906494141, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.201, 'eval_test_samples_per_second': 46.628, 'eval_test_steps_per_second': 5.828, 'epoch': 250.0}
{'loss': 0.5725, 'learning_rate': 1e-06, 'epoch': 300.0}
{'eval_dev_loss': 0.9711431860923767, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1104, 'eval_dev_samples_per_second': 47.383, 'eval_dev_steps_per_second': 6.16, 'epoch': 300.0}
{'eval_test_loss': 0.9804250597953796, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2019, 'eval_test_samples_per_second': 46.593, 'eval_test_steps_per_second': 5.824, 'epoch': 300.0}
{'loss': 0.5914, 'learning_rate': 1e-06, 'epoch': 350.0}
{'eval_dev_loss': 0.903846800327301, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.109, 'eval_dev_samples_per_second': 47.417, 'eval_dev_steps_per_second': 6.164, 'epoch': 350.0}
{'eval_test_loss': 0.9185667037963867, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2049, 'eval_test_samples_per_second': 46.475, 'eval_test_steps_per_second': 5.809, 'epoch': 350.0}
{'loss': 0.5784, 'learning_rate': 1e-06, 'epoch': 400.0}
{'eval_dev_loss': 0.9256725311279297, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1069, 'eval_dev_samples_per_second': 47.463, 'eval_dev_steps_per_second': 6.17, 'epoch': 400.0}
{'eval_test_loss': 1.0020983219146729, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2024, 'eval_test_samples_per_second': 46.573, 'eval_test_steps_per_second': 5.822, 'epoch': 400.0}
{'loss': 0.588, 'learning_rate': 1e-06, 'epoch': 450.0}
{'eval_dev_loss': 0.8773915767669678, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1122, 'eval_dev_samples_per_second': 47.345, 'eval_dev_steps_per_second': 6.155, 'epoch': 450.0}
{'eval_test_loss': 0.9316727519035339, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2026, 'eval_test_samples_per_second': 46.565, 'eval_test_steps_per_second': 5.821, 'epoch': 450.0}
{'loss': 0.597, 'learning_rate': 1e-06, 'epoch': 500.0}
{'eval_dev_loss': 0.9450898170471191, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.109, 'eval_dev_samples_per_second': 47.417, 'eval_dev_steps_per_second': 6.164, 'epoch': 500.0}
{'eval_test_loss': 1.0078532695770264, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2015, 'eval_test_samples_per_second': 46.607, 'eval_test_steps_per_second': 5.826, 'epoch': 500.0}
{'loss': 0.6204, 'learning_rate': 1e-06, 'epoch': 550.0}
{'eval_dev_loss': 0.7803298234939575, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1139, 'eval_dev_samples_per_second': 47.307, 'eval_dev_steps_per_second': 6.15, 'epoch': 550.0}
{'eval_test_loss': 0.8688855767250061, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2032, 'eval_test_samples_per_second': 46.543, 'eval_test_steps_per_second': 5.818, 'epoch': 550.0}
{'loss': 0.6537, 'learning_rate': 1e-06, 'epoch': 600.0}
{'eval_dev_loss': 0.8348448872566223, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1158, 'eval_dev_samples_per_second': 47.264, 'eval_dev_steps_per_second': 6.144, 'epoch': 600.0}
{'eval_test_loss': 0.9373067021369934, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2013, 'eval_test_samples_per_second': 46.616, 'eval_test_steps_per_second': 5.827, 'epoch': 600.0}
{'loss': 0.6396, 'learning_rate': 1e-06, 'epoch': 650.0}
{'eval_dev_loss': 0.8276925683021545, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1126, 'eval_dev_samples_per_second': 47.336, 'eval_dev_steps_per_second': 6.154, 'epoch': 650.0}
{'eval_test_loss': 0.8800839185714722, 'eval_test_model_preparation_time': 0.0014, 'eval_test_runtime': 1.2016, 'eval_test_samples_per_second': 46.606, 'eval_test_steps_per_second': 5.826, 'epoch': 650.0}
{'loss': 0.6836, 'learning_rate': 1e-06, 'epoch': 700.0}
{'eval_dev_loss': 0.7672832608222961, 'eval_dev_model_preparation_time': 0.0014, 'eval_dev_runtime': 2.1109, 'eval_dev_samples_per_second': 47.374, 'eval_dev_steps_per_second': 6.159, 'epoch': 700.0}
