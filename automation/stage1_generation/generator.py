#!/usr/bin/env python3
"""
Stage 1: Synthetic Data Generation Automation
Auto-generate data generation and validation scripts from configuration files
"""

import os
import sys
import yaml
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple

# Add automation directory to path, import unified config
sys.path.insert(0, str(Path(__file__).parent.parent))
from config import PROJECT_ROOT, SYNTHETIC_BASE, ORIGINAL_DATA_DIR

# Add utils directory to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / 'utils'))

# Import experiment manager (Batch solution)
from experiment_manager_batch import (
    BatchExperimentManager,
    compute_parameter_fingerprint,
    save_experiment_metadata
)

# Import API config loader
try:
    from api_config_loader import load_api_config, get_openai_client_code
except ImportError:
    # If not found, provide default implementation
    def load_api_config(config_name="default"):
        return {
            'base_url': 'https://api2.aigcbest.top/v1',
            'timeout': 120
        }

class SyntheticDataGenerator:
    def __init__(self, config_path: str, auto_resolve_conflicts: bool = False):
        """
        Initialize generator

        Args:
            config_path: Configuration file path (YAML)
            auto_resolve_conflicts: Auto-resolve directory conflicts (don't prompt user)
        """
        self.config = self.load_config(config_path)
        self.validate_config()

        self.project_root = PROJECT_ROOT
        self.output_base = SYNTHETIC_BASE
        self.auto_resolve_conflicts = auto_resolve_conflicts

        # Initialize experiment manager (Batch solution)
        self.experiment_manager = BatchExperimentManager(self.output_base)

    def load_config(self, config_path: str) -> Dict:
        """Load configuration file"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def validate_config(self):
        """Validate required fields in configuration file"""
        required_fields = [
            'task_name',
            'dataset',
            'generation'
        ]

        for field in required_fields:
            if field not in self.config:
                raise ValueError(f"Missing required field in config file: {field}")

        # Validate generation section
        gen_fields = ['model', 'temperature', 'rephrase_prompt', 'field_to_rephrase']
        for field in gen_fields:
            if field not in self.config['generation']:
                raise ValueError(f"Missing required field in generation config: {field}")

        # Check generation strategy
        strategy = self.config['generation'].get('strategy', 'two_stage')

        # Validate validation section (optional in direct_all mode)
        if strategy == 'two_stage':
            if 'validation' not in self.config:
                raise ValueError(f"Missing required field in two_stage mode: validation")

            val_fields = ['model', 'validation_prompt', 'few_shot_examples']
            for field in val_fields:
                if field not in self.config['validation']:
                    raise ValueError(f"Missing required field in validation config: {field}")
        elif strategy == 'direct_all':
            # direct_all mode: validation config is optional
            pass
        else:
            raise ValueError(f"Unknown generation strategy: {strategy}. Supported strategies: 'two_stage', 'direct_all'")

    def _get_api_config_code(self, config_name: str = "generation") -> str:
        """
        Get API configuration code (for generated scripts)

        Args:
            config_name: Config name ("generation" or "validation")

        Returns:
            API client initialization code
        """
        # Read API config from generation or validation section
        if config_name == "generation" and 'generation' in self.config:
            api_key = self.config['generation'].get('api_key', 'sk-eWSYPo0CvhRYgcJs55B0C3F00aC74f6e95F47c1f4772292c')
            base_url = self.config['generation'].get('base_url', 'https://api2.aigcbest.top/v1')
            timeout = self.config['generation'].get('timeout', 120)
        elif config_name == "validation" and 'validation' in self.config:
            api_key = self.config['validation'].get('api_key', 'sk-eWSYPo0CvhRYgcJs55B0C3F00aC74f6e95F47c1f4772292c')
            base_url = self.config['validation'].get('base_url', 'https://api2.aigcbest.top/v1')
            timeout = self.config['validation'].get('timeout', 120)
        else:
            # Default values
            api_key = 'sk-eWSYPo0CvhRYgcJs55B0C3F00aC74f6e95F47c1f4772292c'
            base_url = 'https://api2.aigcbest.top/v1'
            timeout = 120

        return f'''# API configuration (read from {config_name} section in config file)
API_KEY = "{api_key}"
API_BASE = "{base_url}"

client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE,
    timeout={timeout}
)'''

    def get_output_dir_name(self) -> Tuple[Path, str]:
        """
        Generate output directory path based on config (Batch solution 3++)

        Directory structure:
        - _shared/{Dataset}/{semantic_dirname}/  # Physical storage
        - batch_{date}_{purpose}/{Dataset}/{semantic_dirname}/  # Logical view (symlink)

        Examples:
        - _shared/Copa/temp07_topp09_gpt4o/  # Actual data
        - batch_20241229_temperature/Copa/temp07_topp09_gpt4o/  # Symlink

        Returns:
            (physical_dir, fingerprint)
            physical_dir: Physical directory path in _shared/ (actual data generation location)
            fingerprint: Parameter fingerprint (for deduplication)
        """
        # Use experiment manager to prepare directory
        output_dir, fingerprint = self.experiment_manager.prepare_experiment_dir(
            self.config,
            auto_resolve=self.auto_resolve_conflicts
        )

        return output_dir, fingerprint

    def generate_rephrase_script(self, output_dir: Path, strategy: str = "all") -> str:
        """
        Generate data rephrasing script

        Args:
            output_dir: Output directory path
            strategy: "all", "top20", "rest"

        Returns:
            Script content
        """
        cfg = self.config
        gen_cfg = cfg['generation']
        dataset_cfg = cfg['dataset']

        # ç¡®å®šæ•°æ®é‡é™åˆ¶
        if strategy == "all":
            limit_code = ""
            output_suffix = ""
        elif strategy == "top20":
            limit_code = """
    if progress > 20:
        break
"""
            output_suffix = "_top20"
        elif strategy == "rest":
            limit_code = """
    if progress <= 20:
        continue
"""
            output_suffix = "_rest"
        else:
            raise ValueError(f"æœªçŸ¥ç­–ç•¥: {strategy}")

        # ç”Ÿæˆè„šæœ¬
        script = f'''#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆçš„åˆæˆæ•°æ®ç”Ÿæˆè„šæœ¬

ä»»åŠ¡: {cfg['task_name']}
è®­ç»ƒæ–¹æ³•: {cfg.get('training_method', 'general')}
ç”Ÿæˆæ¨¡å‹: {gen_cfg['model']}
ç­–ç•¥: {strategy}
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""

from tqdm import tqdm
import os
import json
from openai import OpenAI

{self._get_api_config_code("generation")}

# â­ Few-shot examples placeholder (will be injected by review_top20.py)
# FEWSHOT_EXAMPLES = [...]

def generate_prompt({', '.join(dataset_cfg['fields'])}):
    """ç”Ÿæˆæ”¹å†™æç¤ºè¯"""

    # â­ æ„å»ºfew-shotæ–‡æœ¬ï¼ˆå¦‚æœå­˜åœ¨FEWSHOT_EXAMPLESï¼‰
    fewshot_text = ""
    if 'FEWSHOT_EXAMPLES' in globals() and len(FEWSHOT_EXAMPLES) > 0:
        for i, ex in enumerate(FEWSHOT_EXAMPLES, 1):
            fewshot_text += f"Example {{i}}:\\n"
            fewshot_text += f"Original {gen_cfg['field_to_rephrase']}: {{ex['original']}}\\n"
            fewshot_text += f"Rephrased {gen_cfg['field_to_rephrase']}: {{ex['rephrased']}}\\n"
            # æ·»åŠ å…¶ä»–å­—æ®µä½œä¸ºä¸Šä¸‹æ–‡
            for key in ex:
                if key not in ['original', 'rephrased']:
                    fewshot_text += f"{{key}}: {{ex[key]}}\\n"
            fewshot_text += "\\n"

    # â­ åŸå§‹promptæ¨¡æ¿
    prompt_template = """\\
{gen_cfg['rephrase_prompt']}
"""

    # â­ æ›¿æ¢{{{{REPHRASE_FEWSHOT}}}}å ä½ç¬¦
    prompt = prompt_template.replace("{{{{REPHRASE_FEWSHOT}}}}", fewshot_text)

    # â­ æ›¿æ¢å­—æ®µå€¼ï¼ˆä½¿ç”¨.format()ï¼‰
    return prompt.format({', '.join([f'{f}={f}' for f in dataset_cfg['fields']])})

# åŠ è½½åŸå§‹æ•°æ®
data = []
input_file = "{self.project_root / dataset_cfg['input_path']}"
with open(input_file, 'r', encoding='utf-8') as f:
    for line in f:
        data.append(json.loads(line.strip()))

print(f"åŠ è½½äº† {{len(data)}} æ¡åŸå§‹æ•°æ®")

# å‡†å¤‡è¾“å‡º
# ğŸ†• åˆ›å»ºæ•°æ®é›†å­ç›®å½•
dataset_dir = os.path.join("{output_dir}", "{dataset_cfg.get('dataset_name', cfg.get('task_name', 'Dataset'))}")
os.makedirs(dataset_dir, exist_ok=True)

output_file = os.path.join(dataset_dir, "{dataset_cfg['task_name']}_train{output_suffix}.jsonl")
out_file = open(output_file, "w", encoding='utf-8')

print(f"è¾“å‡ºæ–‡ä»¶: {{output_file}}")

# å¤„ç†æ•°æ®
progress = 0
for i in tqdm(range(len(data))):
    progress += 1{limit_code}

    # æ„é€ æç¤ºè¯
    prompt_args = {{{', '.join([f'"{f}": data[i]["{f}"]' for f in dataset_cfg['fields']])}}}
    prompt = generate_prompt(**prompt_args)

    # è°ƒç”¨ API
    try:
        response = client.chat.completions.create(
            model="{gen_cfg['model']}",
            messages=[
                {{"role": "user", "content": prompt}}
            ],
            temperature={gen_cfg['temperature']}
        )

        # æå–ç»“æœ
        rephrased_text = response.choices[0].message.content.strip()

        # æ„é€ è¾“å‡º
        result = data[i].copy()
        result["{gen_cfg['field_to_rephrase']}"] = rephrased_text

        # å†™å…¥æ–‡ä»¶
        out_file.write(json.dumps(result, ensure_ascii=False) + "\\n")
        out_file.flush()

    except Exception as e:
        print(f"\\nå¤„ç†ç¬¬ {{i}} æ¡æ•°æ®æ—¶å‡ºé”™: {{e}}")
        # å‡ºé”™æ—¶ä½¿ç”¨åŸå§‹æ•°æ®
        out_file.write(json.dumps(data[i], ensure_ascii=False) + "\\n")
        out_file.flush()

out_file.close()
print(f"\\nå®Œæˆ! è¾“å‡º: {{output_file}}")
'''
        return script

    def generate_validation_script(self, output_dir: Path) -> str:
        """ç”ŸæˆéªŒè¯è„šæœ¬"""
        cfg = self.config
        val_cfg = cfg['validation']
        dataset_cfg = cfg['dataset']
        gen_cfg = cfg['generation']

        # ä»é…ç½®è¯»å–field_to_rephrase
        field_to_rephrase = gen_cfg['field_to_rephrase']

        script = f'''#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆçš„åˆæˆæ•°æ®éªŒè¯è„šæœ¬ï¼ˆæ‹’ç»é‡‡æ ·ï¼‰

ä»»åŠ¡: {cfg['task_name']}
è®­ç»ƒæ–¹æ³•: {cfg.get('training_method', 'general')}
éªŒè¯æ¨¡å‹: {val_cfg['model']}
Field to rephrase: {field_to_rephrase}
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""

from tqdm import tqdm
import os
import json
from openai import OpenAI

{self._get_api_config_code("validation")}

# â­ å°è¯•ä»validation_checkpointsåŠ è½½è‡ªåŠ¨ç”Ÿæˆçš„few-shot
# ï¼ˆç”±annotate_samples.pyç”Ÿæˆï¼‰
VALIDATION_FEWSHOT_EXAMPLES = []
try:
    import sys
    from pathlib import Path
    checkpoint_file = Path(__file__).parent.parent / "validation_checkpoints" / "validation_fewshot.json"
    if checkpoint_file.exists():
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            fewshot_data = json.load(f)
            VALIDATION_FEWSHOT_EXAMPLES = fewshot_data.get('examples', [])
        print(f"âœ“ åŠ è½½äº† {{len(VALIDATION_FEWSHOT_EXAMPLES)}} ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„validation few-shot examples")
except Exception as e:
    print(f"âš ï¸  æœªæ‰¾åˆ°è‡ªåŠ¨ç”Ÿæˆçš„few-shotï¼Œå°†ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„few-shot: {{e}}")

def generate_validation_prompt({', '.join(['original_' + f for f in dataset_cfg['fields']] + ['rephrased_' + field_to_rephrase])}):
    """ç”ŸæˆéªŒè¯æç¤ºè¯"""

    # â­ æ„å»ºfew-shotæ–‡æœ¬
    fewshot_text = ""

    # ä¼˜å…ˆä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„few-shotï¼ˆæ¥è‡ªæ ·æœ¬21-40ï¼‰
    if len(VALIDATION_FEWSHOT_EXAMPLES) > 0:
        for i, ex in enumerate(VALIDATION_FEWSHOT_EXAMPLES, 1):
            fewshot_text += f"Example {{i}}:\\n"
            fewshot_text += f"Original {field_to_rephrase}: {{ex.get('original_{field_to_rephrase}', 'N/A')}}\\n"
            fewshot_text += f"Rephrased {field_to_rephrase}: {{ex.get('rephrased_{field_to_rephrase}', 'N/A')}}\\n"
            # æ·»åŠ å…¶ä»–å­—æ®µ
            for key in ex:
                if not key.startswith('original_') and not key.startswith('rephrased_') and key != 'evaluation':
                    fewshot_text += f"{{key}}: {{ex[key]}}\\n"
            fewshot_text += f"Evaluation: {{ex.get('evaluation', 'same')}}\\n\\n"
    else:
        # å¤‡ç”¨ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æ‰‹åŠ¨æä¾›çš„few-shot
        manual_examples = {val_cfg.get('few_shot_examples', [])}
        for i, ex in enumerate(manual_examples, 1):
            if isinstance(ex, dict):
                fewshot_text += f"Example {{i}}:\\n"
                for k, v in ex.items():
                    fewshot_text += f"{{k}}: {{v}}\\n"
                fewshot_text += "\\n"

    # â­ åŸå§‹promptæ¨¡æ¿
    prompt_template = """\\
{val_cfg['validation_prompt']}
"""

    # â­ æ›¿æ¢{{{{VALIDATION_FEWSHOT}}}}å ä½ç¬¦
    prompt = prompt_template.replace("{{{{VALIDATION_FEWSHOT}}}}", fewshot_text)

    # â­ æ„å»ºå­—æ®µå­—å…¸ç”¨äºformat
    format_dict = {{}}
    for field in {dataset_cfg['fields']}:
        format_dict[f'original_{{field}}'] = locals().get(f'original_{{field}}', '')
    format_dict['rephrased_{field_to_rephrase}'] = locals().get('rephrased_{field_to_rephrase}', '')

    # â­ æ›¿æ¢å­—æ®µå€¼
    return prompt.format(**format_dict)
"""

# åŠ è½½åŸå§‹æ•°æ®
original_data = []
with open("{self.project_root / dataset_cfg['input_path']}", 'r', encoding='utf-8') as f:
    for line in f:
        original_data.append(json.loads(line.strip()))

# åŠ è½½åˆæˆæ•°æ®
# ğŸ†• ä»æ•°æ®é›†å­ç›®å½•è¯»å–
dataset_dir = os.path.join("{output_dir}", "{dataset_cfg.get('dataset_name', cfg.get('task_name', 'Dataset'))}")
synthetic_data = []
synthetic_file = os.path.join(dataset_dir, "{dataset_cfg['task_name']}_train.jsonl")
with open(synthetic_file, 'r', encoding='utf-8') as f:
    for line in f:
        synthetic_data.append(json.loads(line.strip()))

print(f"åŸå§‹æ•°æ®: {{len(original_data)}} æ¡")
print(f"åˆæˆæ•°æ®: {{len(synthetic_data)}} æ¡")

if len(original_data) != len(synthetic_data):
    print("âš  è­¦å‘Š: æ•°æ®é‡ä¸åŒ¹é…!")

# å‡†å¤‡è¾“å‡ºï¼ˆä¸´æ—¶æ–‡ä»¶ï¼‰
temp_output_file = os.path.join(dataset_dir, "{dataset_cfg['task_name']}_train_validated.jsonl")
out_file = open(temp_output_file, "w", encoding='utf-8')

correct_count = 0
total_count = 0

# éªŒè¯æ¯æ¡æ•°æ®
for i in tqdm(range(min(len(original_data), len(synthetic_data)))):
    original = original_data[i]
    synthetic = synthetic_data[i]

    # ğŸ”´ æ’é™¤æ ·æœ¬21-40ï¼ˆç´¢å¼•20-39ï¼‰
    # è¿™äº›æ ·æœ¬ç”¨ä½œjudgerçš„few-shot examplesï¼Œä¸åº”è¢«judgeréªŒè¯ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
    if 20 <= i < 40:
        # ç›´æ¥ä½¿ç”¨åˆæˆæ•°æ®ï¼Œä¸ç»è¿‡judgeréªŒè¯
        out_file.write(json.dumps(synthetic, ensure_ascii=False) + "\\n")
        correct_count += 1
        total_count += 1
        out_file.flush()
        continue

    # æ„é€ éªŒè¯æç¤ºè¯
    prompt_args = {{}}
    for field in {dataset_cfg['fields']}:
        prompt_args[f'original_{{field}}'] = original[field]
    prompt_args['rephrased_{gen_cfg['field_to_rephrase']}'] = synthetic['{gen_cfg['field_to_rephrase']}']

    prompt = generate_validation_prompt(**prompt_args)

    try:
        response = client.chat.completions.create(
            model="{val_cfg['model']}",
            messages=[
                {{"role": "system", "content": "You are a helpful judge."}},
                {{"role": "user", "content": prompt}}
            ],
            temperature=0.0
        )

        result = response.choices[0].message.content.strip().lower()

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡éªŒè¯
        if 'not the same' in result or 'not same' in result:
            # éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
            out_file.write(json.dumps(original, ensure_ascii=False) + "\\n")
        else:
            # éªŒè¯æˆåŠŸï¼Œä½¿ç”¨åˆæˆæ•°æ®
            out_file.write(json.dumps(synthetic, ensure_ascii=False) + "\\n")
            correct_count += 1

        total_count += 1
        out_file.flush()

    except Exception as e:
        print(f"\\néªŒè¯ç¬¬ {{i}} æ¡æ•°æ®æ—¶å‡ºé”™: {{e}}")
        # å‡ºé”™æ—¶ä½¿ç”¨åŸå§‹æ•°æ®
        out_file.write(json.dumps(original, ensure_ascii=False) + "\\n")
        total_count += 1
        out_file.flush()

out_file.close()

accuracy = correct_count / total_count if total_count > 0 else 0
print(f"\\néªŒè¯å®Œæˆ!")
print(f"é€šè¿‡ç‡: {{correct_count}}/{{total_count}} = {{accuracy:.2%}}")
print(f"ä¸´æ—¶è¾“å‡ºæ–‡ä»¶: {{temp_output_file}}")

# ğŸ†• æœ€ç»ˆåŒ–æ•°æ®é›†ï¼šé‡å‘½åvalidatedæ–‡ä»¶ + å¤åˆ¶validation/test
print("\\næœ€ç»ˆåŒ–æ•°æ®é›†...")
import shutil

# 1. å°†validatedæ–‡ä»¶é‡å‘½åä¸ºæ­£å¼çš„trainæ–‡ä»¶
final_train_file = os.path.join(dataset_dir, "{dataset_cfg['task_name']}_train.jsonl")
if os.path.exists(final_train_file):
    os.remove(final_train_file)  # åˆ é™¤åŸå§‹çš„æœªéªŒè¯æ–‡ä»¶
shutil.move(temp_output_file, final_train_file)
print(f"âœ“ è®­ç»ƒé›†: {{final_train_file}}")

# 2. å¤åˆ¶validationå’Œtestæ–‡ä»¶fromåŸå§‹æ•°æ®é›†
original_dir = "{self.project_root / dataset_cfg.get('original_dir', dataset_cfg['input_path'].rsplit('/', 1)[0])}"
files_config = {dataset_cfg.get('files', {})}

# å¤åˆ¶validationæ–‡ä»¶
if 'validation' in files_config:
    val_file = files_config['validation']
    src_val = os.path.join(original_dir, val_file)
    dst_val = os.path.join(dataset_dir, val_file)
    if os.path.exists(src_val):
        shutil.copy2(src_val, dst_val)
        print(f"âœ“ éªŒè¯é›†: {{dst_val}}")
    else:
        print(f"âš   è­¦å‘Š: éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨: {{src_val}}")

# å¤åˆ¶testæ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
if 'test' in files_config:
    test_file = files_config['test']
    src_test = os.path.join(original_dir, test_file)
    dst_test = os.path.join(dataset_dir, test_file)
    if os.path.exists(src_test):
        shutil.copy2(src_test, dst_test)
        print(f"âœ“ æµ‹è¯•é›†: {{dst_test}}")

print(f"\\nâœ… æ•°æ®é›†å·²å®Œæˆï¼å¯ç”¨äºMeZOè®­ç»ƒï¼š")
print(f"   python PromptZO/MeZO/large_models/run.py --task {{dataset_dir}}")
'''
        return script

    def create_config_copy(self, output_dir: Path):
        """ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬åˆ°è¾“å‡ºç›®å½•"""
        config_copy_path = output_dir / "generation_config.yaml"
        with open(config_copy_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config, f, allow_unicode=True, default_flow_style=False)
        print(f"âœ“ é…ç½®å‰¯æœ¬: {config_copy_path}")

    def generate_all(self):
        """ç”Ÿæˆæ‰€æœ‰è„šæœ¬"""
        print("\n" + "="*80)
        print(f"åˆæˆæ•°æ®ç”Ÿæˆè„šæœ¬è‡ªåŠ¨ç”Ÿæˆå™¨")
        print("="*80)

        # è·å–å®éªŒä¿¡æ¯
        experiment_cfg = self.config.get('experiment', {})
        experiment_purpose = experiment_cfg.get('purpose', 'general')
        experiment_desc = experiment_cfg.get('description', '')

        # â­ è·å–ç”Ÿæˆç­–ç•¥
        gen_strategy = self.config['generation'].get('strategy', 'two_stage')

        print(f"ç”Ÿæˆç­–ç•¥: {gen_strategy}")
        print(f"å®éªŒç›®çš„: {experiment_purpose}")
        if experiment_desc:
            print(f"å®éªŒæè¿°: {experiment_desc}")
        print(f"ä»»åŠ¡: {self.config['task_name']}")
        print(f"è®­ç»ƒæ–¹æ³•: {self.config.get('training_method', 'general')}")
        print(f"ç”Ÿæˆæ¨¡å‹: {self.config['generation']['model']}")

        # â­ validationæ¨¡å‹ä¿¡æ¯ï¼ˆä»…åœ¨two_stageæ¨¡å¼ä¸‹æ˜¾ç¤ºï¼‰
        if gen_strategy == 'two_stage' and 'validation' in self.config:
            print(f"éªŒè¯æ¨¡å‹: {self.config['validation']['model']}")

        print("="*80)

        # ä½¿ç”¨å®éªŒç®¡ç†å™¨å‡†å¤‡è¾“å‡ºç›®å½•ï¼ˆåŒ…å«å†²çªæ£€æµ‹ï¼‰
        output_dir, fingerprint = self.get_output_dir_name()
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"\nè¾“å‡ºç›®å½•: {output_dir.relative_to(self.project_root)}")
        print(f"å‚æ•°æŒ‡çº¹: {fingerprint}")

        # ğŸ†• åˆ›å»ºæ•°æ®é›†å­ç›®å½•ï¼ˆç”¨äºå­˜æ”¾æ•°æ®æ–‡ä»¶ï¼‰
        dataset_cfg = self.config['dataset']
        dataset_name = dataset_cfg.get('dataset_name', self.config.get('task_name', 'Dataset'))
        dataset_dir = output_dir / dataset_name
        dataset_dir.mkdir(exist_ok=True)
        print(f"æ•°æ®é›†ç›®å½•: {dataset_dir.relative_to(self.project_root)}")

        # åˆ›å»º scripts å­ç›®å½•
        scripts_dir = output_dir / "scripts"
        scripts_dir.mkdir(exist_ok=True)

        # â­ æ ¹æ®ç”Ÿæˆç­–ç•¥ç”Ÿæˆä¸åŒçš„è„šæœ¬
        print("\nç”Ÿæˆæ”¹å†™è„šæœ¬...")

        if gen_strategy == 'direct_all':
            # ğŸ”¥ direct_all æ¨¡å¼ï¼šåªç”Ÿæˆ rephrase_all.py
            script_path = scripts_dir / "rephrase_all.py"
            script_content = self.generate_rephrase_script(output_dir, "all")

            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(script_content)

            os.chmod(script_path, 0o755)
            print(f"  âœ“ {script_path.name}")
            print(f"  (direct_all æ¨¡å¼ï¼šè·³è¿‡ top20 å’Œ rest è„šæœ¬)")

        else:  # two_stage æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            # ç”Ÿæˆ all, top20, rest ä¸‰ä¸ªè„šæœ¬
            for strategy in ["all", "top20", "rest"]:
                script_path = scripts_dir / f"rephrase_{strategy}.py"
                script_content = self.generate_rephrase_script(output_dir, strategy)

                with open(script_path, 'w', encoding='utf-8') as f:
                    f.write(script_content)

                os.chmod(script_path, 0o755)
                print(f"  âœ“ {script_path.name}")

        # â­ ç”ŸæˆéªŒè¯è„šæœ¬ï¼ˆä»…åœ¨ two_stage æ¨¡å¼ä¸”é…ç½®äº† validation æ—¶ï¼‰
        if gen_strategy == 'two_stage' and 'validation' in self.config:
            print("\nç”ŸæˆéªŒè¯è„šæœ¬...")
            val_script_path = scripts_dir / "validate.py"
            val_script_content = self.generate_validation_script(output_dir)

            with open(val_script_path, 'w', encoding='utf-8') as f:
                f.write(val_script_content)

            os.chmod(val_script_path, 0o755)
            print(f"  âœ“ {val_script_path.name}")
        elif gen_strategy == 'direct_all':
            print("\nè·³è¿‡éªŒè¯è„šæœ¬ç”Ÿæˆï¼ˆdirect_all æ¨¡å¼ï¼‰")

        # ä¿å­˜é…ç½®å‰¯æœ¬
        print("\nä¿å­˜é…ç½®...")
        self.create_config_copy(output_dir)

        # ä¿å­˜å®éªŒå…ƒæ•°æ®
        metadata_path = save_experiment_metadata(output_dir, self.config, fingerprint)
        if metadata_path:
            print(f"âœ“ å®éªŒå…ƒæ•°æ®: {metadata_path.relative_to(self.project_root)}")

        # ç”Ÿæˆ README
        readme_content = self.generate_readme(output_dir, fingerprint)
        readme_path = output_dir / "README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        print(f"âœ“ README: {readme_path.relative_to(self.project_root)}")

        print("\\n" + "="*80)
        print("ç”Ÿæˆå®Œæˆï¼")
        print("="*80)
        print(f"\\nè„šæœ¬ä½ç½®: {scripts_dir}")

        # â­ æ ¹æ®ç­–ç•¥æ˜¾ç¤ºä¸åŒçš„ä½¿ç”¨è¯´æ˜
        if gen_strategy == 'direct_all':
            print(f"\\nä½¿ç”¨æ–¹æ³• (direct_all æ¨¡å¼):")
            print(f"  1. è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY=your-key")
            print(f"  2. ç›´æ¥è¿è¡Œç”Ÿæˆ: python {scripts_dir}/rephrase_all.py")
            print(f"  3. ç”Ÿæˆå®Œæˆåï¼Œæ•°æ®ä¿å­˜åœ¨: {dataset_dir}")
        else:
            print(f"\\nä½¿ç”¨æ–¹æ³• (two_stage æ¨¡å¼):")
            print(f"  1. è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY=your-key")
            print(f"  2. è¿è¡Œç”Ÿæˆ: python {scripts_dir}/rephrase_all.py")
            print(f"  3. è¿è¡ŒéªŒè¯: python {scripts_dir}/validate.py")

    def generate_readme(self, output_dir: Path, fingerprint: str) -> str:
        """ç”Ÿæˆ README æ–‡æ¡£"""
        cfg = self.config
        experiment_cfg = cfg.get('experiment', {})
        gen_strategy = cfg['generation'].get('strategy', 'two_stage')

        # â­ æ„å»ºvalidationæ¨¡å‹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        val_model_info = ""
        if 'validation' in cfg:
            val_model_info = f"\n- **éªŒè¯æ¨¡å‹**: {cfg['validation']['model']}"

        return f"""# {cfg['task_name']} åˆæˆæ•°æ®ç”Ÿæˆ

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## å®éªŒä¿¡æ¯

- **å®éªŒç›®çš„**: {experiment_cfg.get('purpose', 'general')}
- **å®éªŒID**: {experiment_cfg.get('experiment_id', 'N/A')}
- **å®éªŒæè¿°**: {experiment_cfg.get('description', 'N/A')}
- **å‚æ•°æŒ‡çº¹**: {fingerprint}

## é…ç½®ä¿¡æ¯

- **ç”Ÿæˆç­–ç•¥**: {gen_strategy}
- **ä»»åŠ¡**: {cfg['task_name']}
- **è®­ç»ƒæ–¹æ³•**: {cfg.get('training_method', 'general')}
- **æ•°æ®é›†**: {cfg['dataset']['task_name']}
- **ç”Ÿæˆæ¨¡å‹**: {cfg['generation']['model']}
- **Temperature**: {cfg['generation']['temperature']}{val_model_info}
- **ç‰ˆæœ¬**: {cfg.get('version', 'v1')}

## ç›®å½•ç»“æ„

```
{output_dir.name}/
â”œâ”€â”€ {cfg['dataset'].get('dataset_name', cfg['task_name'])}/     # ğŸ†• æ•°æ®é›†ç›®å½•ï¼ˆMeZOå¯ç›´æ¥ä½¿ç”¨ï¼‰
â”‚   â”œâ”€â”€ {cfg['dataset']['task_name']}_train.jsonl              # åˆæˆ+éªŒè¯åçš„è®­ç»ƒé›†
â”‚   â”œâ”€â”€ {cfg['dataset']['task_name']}_validation.jsonl         # éªŒè¯é›†ï¼ˆå¤åˆ¶è‡ªåŸå§‹ï¼‰
â”‚   â””â”€â”€ {cfg['dataset']['task_name']}_test.jsonl               # æµ‹è¯•é›†ï¼ˆå¤åˆ¶è‡ªåŸå§‹ï¼‰
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ rephrase_all.py      # æ”¹å†™å…¨éƒ¨æ•°æ®
â”‚   â”œâ”€â”€ rephrase_top20.py    # æ”¹å†™å‰20ä¸ªå›°éš¾æ ·æœ¬
â”‚   â”œâ”€â”€ rephrase_rest.py     # æ”¹å†™å‰©ä½™æ ·æœ¬
â”‚   â””â”€â”€ validate.py          # éªŒè¯è„šæœ¬ï¼ˆæ‹’ç»é‡‡æ ·+æ•°æ®é›†æœ€ç»ˆåŒ–ï¼‰
â”œâ”€â”€ generation_config.yaml   # é…ç½®æ–‡ä»¶å‰¯æœ¬
â”œâ”€â”€ experiment_metadata.json # å®éªŒå…ƒæ•°æ®
â””â”€â”€ README.md               # æœ¬æ–‡ä»¶
```

## ä½¿ç”¨æ–¹æ³•

### 1. è®¾ç½®ç¯å¢ƒå˜é‡

```bash
export OPENAI_API_KEY="your-api-key"
export OPENAI_API_BASE="https://api.openai.com/v1"  # å¯é€‰
```

### 2. ç”Ÿæˆåˆæˆæ•°æ®

```bash
{"# direct_all æ¨¡å¼ï¼šç›´æ¥ç”Ÿæˆå…¨éƒ¨æ•°æ®" if gen_strategy == 'direct_all' else "# æ–¹å¼1: æ”¹å†™å…¨éƒ¨æ•°æ®"}
python scripts/rephrase_all.py
{"" if gen_strategy == 'direct_all' else '''
# æ–¹å¼2: åˆ†åˆ«æ”¹å†™å›°éš¾æ ·æœ¬å’Œå‰©ä½™æ ·æœ¬
python scripts/rephrase_top20.py
python scripts/rephrase_rest.py'''}
```
{"" if gen_strategy == 'direct_all' else '''
### 3. éªŒè¯æ•°æ®è´¨é‡å¹¶æœ€ç»ˆåŒ–æ•°æ®é›†

```bash
python scripts/validate.py
```

æ­¤è„šæœ¬ä¼šï¼š
1. ä½¿ç”¨rejection samplingéªŒè¯åˆæˆæ•°æ®è´¨é‡
2. å°†éªŒè¯é€šè¿‡çš„æ•°æ®é‡å‘½åä¸ºæ­£å¼è®­ç»ƒé›†
3. ä»åŸå§‹æ•°æ®é›†å¤åˆ¶validationå’Œtestæ–‡ä»¶
4. ç”Ÿæˆå®Œæ•´çš„MeZOå¯ç”¨æ•°æ®é›†
'''}
### {"3" if gen_strategy == 'direct_all' else "4"}. ä½¿ç”¨æ•°æ®é›†è®­ç»ƒæ¨¡å‹

```bash
# ä½¿ç”¨MeZOè®­ç»ƒ
python PromptZO/MeZO/large_models/run.py \\
    --task {cfg['dataset'].get('dataset_name', cfg['task_name'])} \\
    --model meta-llama/Llama-3.2-1B \\
    --num_train_epochs 3 \\
    --per_device_train_batch_size 4
```

## æœ€ç»ˆæ•°æ®é›†ç»“æ„

```
{cfg['dataset'].get('dataset_name', cfg['task_name'])}/
â”œâ”€â”€ {cfg['dataset']['task_name']}_train.jsonl       # {"åˆæˆæ•°æ®" if gen_strategy == 'direct_all' else "åˆæˆ+éªŒè¯åçš„è®­ç»ƒé›†"}
â”œâ”€â”€ {cfg['dataset']['task_name']}_validation.jsonl  # éªŒè¯é›†ï¼ˆæ¥è‡ªåŸå§‹æ•°æ®ï¼‰
â””â”€â”€ {cfg['dataset']['task_name']}_test.jsonl        # æµ‹è¯•é›†ï¼ˆæ¥è‡ªåŸå§‹æ•°æ®ï¼‰
```

æ­¤ç›®å½•å¯ä»¥ç›´æ¥ä¼ é€’ç»™MeZOè®­ç»ƒè„šæœ¬ä½¿ç”¨ã€‚

## Prompt ä¿¡æ¯

### æ”¹å†™ Prompt

```
{cfg['generation']['rephrase_prompt'][:200]}...
```
{"" if gen_strategy == 'direct_all' else f'''
### éªŒè¯ Prompt

```
{cfg.get('validation', {}).get('validation_prompt', 'N/A')[:200]}...
```
'''}
è¯¦è§ `generation_config.yaml`
"""

def main():
    import argparse

    parser = argparse.ArgumentParser(description="åˆæˆæ•°æ®ç”Ÿæˆè„šæœ¬è‡ªåŠ¨ç”Ÿæˆå™¨")
    parser.add_argument("config", help="é…ç½®æ–‡ä»¶è·¯å¾„ (YAML)")
    parser.add_argument(
        "--auto-resolve",
        action="store_true",
        help="è‡ªåŠ¨è§£å†³ç›®å½•å†²çªï¼ˆä¸æç¤ºç”¨æˆ·ï¼‰"
    )
    args = parser.parse_args()

    generator = SyntheticDataGenerator(args.config, auto_resolve_conflicts=args.auto_resolve)
    generator.generate_all()

if __name__ == "__main__":
    main()
