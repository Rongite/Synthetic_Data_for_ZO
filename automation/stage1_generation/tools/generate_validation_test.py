#!/usr/bin/env python3
"""
Generate Validation Prompt Test Script - Checkpoint 3 Tool

Automatically generate test script for validation prompt accuracy based on few-shot and test_set generated by annotate_samples.py.

Features:
  1. Read validation_fewshot.json (qualified samples 21-40)
  2. Read validation_test_set.json (all samples 41-80 with ground_truth)
  3. Generate validate_prompt_test.py test script

Usage:
  cd Data_v2/synthetic/_shared/{Dataset}/{experiment_dir}/scripts/
  python /path/to/automation/stage1_generation/tools/generate_validation_test.py
"""

import json
import sys
from pathlib import Path


TEMPLATE = '''#!/usr/bin/env python3
"""
Validation Prompt Test Script

Auto-generated at: {timestamp}
Purpose: Test validation prompt accuracy on test_set

Usage:
  export OPENAI_API_KEY="your-key"
  python validate_prompt_test.py
"""

import json
import os
from openai import OpenAI

# Initialize OpenAI client
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url=os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1")
)

# Few-shot examples (from qualified samples 21-40)
FEWSHOT_EXAMPLES = {fewshot_json}

# Test set (from all samples 41-80, includes ground_truth)
TEST_SET = {testset_json}

# Validation prompt template
def generate_validation_prompt({prompt_args}):
    """Generate validation prompt"""

    # Build few-shot part
    fewshot_text = ""
    for i, ex in enumerate(FEWSHOT_EXAMPLES, 1):
        fewshot_text += f"""
Example {{i}}:
Original {field_name}: {{ex['original_{field_name}']}}
Rephrased {field_name}: {{ex['rephrased_{field_name}']}}
Evaluation: {{ex['evaluation']}}
"""

    prompt = f"""You are a helpful judge to evaluate if a rephrased {field_name} maintains the same meaning as the original {field_name}.

Your task:
- Compare the original and rephrased {field_name}
- Determine if they convey the same meaning
- Respond with ONLY "same" or "not the same"

Here are some examples:
{{fewshot_text}}

Now judge the following:
Original {field_name}: {{original_{field_name}}}
Rephrased {field_name}: {{rephrased_{field_name}}}

Evaluation:"""

    return prompt


def test_validation_prompt():
    """Test validation prompt accuracy"""
    print("=" * 80)
    print("Testing Validation Prompt Accuracy")
    print("=" * 80)
    print(f"Test set size: {{len(TEST_SET)}} samples")
    print(f"Few-shot examples: {{len(FEWSHOT_EXAMPLES)}} examples")
    print()

    correct = 0
    errors = []

    for i, test_item in enumerate(TEST_SET):
        print(f"\\rTest progress: {{i+1}}/{{len(TEST_SET)}}", end="", flush=True)

        # Generate prompt
        prompt = generate_validation_prompt(
            original_{field_name}=test_item['original_{field_name}'],
            rephrased_{field_name}=test_item['rephrased_{field_name}']
        )

        try:
            # Call AI judge
            response = client.chat.completions.create(
                model="{model}",
                messages=[
                    {{"role": "system", "content": "You are a helpful judge."}},
                    {{"role": "user", "content": prompt}}
                ],
                temperature=0.0
            )

            result = response.choices[0].message.content.strip().lower()
            ground_truth = test_item['ground_truth'].lower()

            # Determine if correct
            if ('same' in result and 'not' not in result and ground_truth == 'same') or \\
               ('not' in result and 'same' in result and ground_truth == 'not the same'):
                correct += 1
            else:
                errors.append({{
                    'index': test_item['index'],
                    'predicted': result,
                    'ground_truth': ground_truth,
                    'original': test_item['original_{field_name}'],
                    'rephrased': test_item['rephrased_{field_name}']
                }})

        except Exception as e:
            print(f"\\nError: Error testing sample {{test_item['index']}}: {{e}}")
            errors.append({{
                'index': test_item['index'],
                'predicted': 'ERROR',
                'ground_truth': test_item['ground_truth'],
                'error': str(e)
            }})

    print()  # New line
    print("=" * 80)
    print("Test Results")
    print("=" * 80)
    print(f"Total: {{len(TEST_SET)}} samples")
    print(f"Correct: {{correct}} samples")
    print(f"Errors: {{len(errors)}} samples")

    accuracy = correct / len(TEST_SET) if TEST_SET else 0
    print(f"\\nAccuracy: {{accuracy:.2%}}")

    if accuracy >= 0.95:
        print("\\n✅ Prompt passed (≥95%)!")

        # Create pass flag
        checkpoint_dir = Path("../validation_checkpoints")
        checkpoint_dir.mkdir(exist_ok=True)
        flag_file = checkpoint_dir / "prompt_test_passed.flag"

        with open(flag_file, 'w') as f:
            f.write(f"Validation prompt test passed\\n")
            f.write(f"Accuracy: {{accuracy:.2%}}\\n")
            f.write(f"Test time: {{__import__('datetime').datetime.now()}}\\n")

        print(f"✓ Created pass flag: {{flag_file}}")
        print("\\nNext step: Run validate.py to verify all 400 samples")
    else:
        print(f"\\n❌ Prompt did not pass (requires ≥95%, current {{accuracy:.2%}})")
        print("\\nPlease adjust validation_prompt and retest")

    # Display error samples
    if errors:
        print("\\nError sample details:")
        for err in errors[:10]:  # Show only first 10
            print(f"\\nSample {{err['index']}}:")
            print(f"  Predicted: {{err.get('predicted', 'N/A')}}")
            print(f"  Actual: {{err['ground_truth']}}")
            if 'original' in err:
                print(f"  Original: {{err['original'][:100]}}...")
                print(f"  Rephrased: {{err['rephrased'][:100]}}...")

        if len(errors) > 10:
            print(f"\\n... {{len(errors)-10}} more error samples not shown")

    return accuracy


if __name__ == "__main__":
    try:
        accuracy = test_validation_prompt()
        sys.exit(0 if accuracy >= 0.95 else 1)
    except KeyboardInterrupt:
        print("\\n\\nUser interrupted")
        sys.exit(1)
    except Exception as e:
        print(f"\\n❌ Error: {{e}}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
'''


def main():
    """Main process"""
    # Detect current directory
    current_dir = Path.cwd()

    # Find necessary files
    checkpoint_dir = current_dir.parent / "validation_checkpoints"
    fewshot_file = checkpoint_dir / "validation_fewshot.json"
    testset_file = checkpoint_dir / "validation_test_set.json"

    # Validate file existence
    if not fewshot_file.exists():
        print(f"❌ Error: Few-shot file does not exist: {fewshot_file}")
        print("   Please run first: python annotate_samples.py --range 21-40")
        return 1

    if not testset_file.exists():
        print(f"❌ Error: Test set file does not exist: {testset_file}")
        print("   Please run first: python annotate_samples.py --range 41-80")
        return 1

    # Read config
    config_file = current_dir.parent / "generation_config.yaml"
    if config_file.exists():
        import yaml
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        val_cfg = config.get('validation', {})
        model = val_cfg.get('model', 'gpt-4o')
    else:
        model = 'gpt-4o'
        print(f"⚠️  Config file does not exist, using default model: {model}")

    # Load few-shot and test_set
    print("Loading data...")
    with open(fewshot_file, 'r', encoding='utf-8') as f:
        fewshot_data = json.load(f)

    with open(testset_file, 'r', encoding='utf-8') as f:
        testset_data = json.load(f)

    field_to_rephrase = fewshot_data.get('field_to_rephrase', 'premise')
    fewshot_examples = fewshot_data.get('examples', [])
    test_set = testset_data.get('test_set', [])

    print(f"  Field: {field_to_rephrase}")
    print(f"  Few-shot examples: {len(fewshot_examples)} examples")
    print(f"  Test set: {len(test_set)} samples")

    if len(fewshot_examples) < 5:
        print(f"\n⚠️  Warning: Only {len(fewshot_examples)} few-shot examples, may be insufficient")

    if len(test_set) < 20:
        print(f"\n⚠️  Warning: Only {len(test_set)} test set samples, may be insufficient for accurate evaluation")

    # Generate script
    print("\nGenerating test script...")

    from datetime import datetime
    script_content = TEMPLATE.format(
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        fewshot_json=json.dumps(fewshot_examples, ensure_ascii=False, indent=4),
        testset_json=json.dumps(test_set, ensure_ascii=False, indent=4),
        field_name=field_to_rephrase,
        prompt_args=f"original_{field_to_rephrase}, rephrased_{field_to_rephrase}",
        model=model
    )

    # Save script
    output_file = current_dir / "validate_prompt_test.py"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(script_content)

    # Set executable permission
    output_file.chmod(0o755)

    print(f"✓ Test script generated: {output_file}")
    print("\n" + "=" * 80)
    print("✅ Generation completed!")
    print("=" * 80)
    print("\nNext steps:")
    print("  1. Set API key: export OPENAI_API_KEY='your-key'")
    print("  2. Run test: python validate_prompt_test.py")
    print("  3. If accuracy < 95%, adjust validation_prompt in config file")
    print("  4. Regenerate script and test until accuracy ≥ 95%")

    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\nUser interrupted")
        sys.exit(1)
    except Exception as e:
        print(f"\n❌ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
