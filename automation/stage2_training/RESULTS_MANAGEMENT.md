# trainingResults ManagementSystem

## ğŸ”´ Core design philosophy: Stage 1 and Stage 2 experiment purposes are independently established

### **Why do we need to classify?**

**Stage 1ï¼ˆDataGenerateï¼‰Experimentobjective**ï¼š
- Answerï¼š" as whatGeneratethisDataï¼Ÿ"
- Exampleï¼š`prompt_engineering`, `temperature_study`, `data_quality_optimization`
- storagelocationï¼š`Data_v2/synthetic/{DataGenerateobjective}/`

**Stage 2ï¼ˆModel Trainingï¼‰Experimentobjective**ï¼š
- Answerï¼š"Why conduct this training?"
- Exampleï¼š`model_comparison`, `hyperparameter_tuning`, `baseline_comparison`
- storagelocationï¼š`Results_v2/{trainingobjective}/`

### **Typical scenario**

```
ã€Scenarioã€‘ï¼šUse the same dataset to conduct multiple types of different training experiments

Datasetï¼ˆStage 1ï¼‰ï¼š
Data_v2/synthetic/prompt_engineering/copa_mezo_v1/
â†‘ DataGenerateobjectiveï¼šTestprompt for DataqualityImpact

trainingExperimentï¼ˆStage 2ï¼‰ï¼š
â”œâ”€â”€ Results_v2/model_comparison/        â† trainingobjectiveï¼šcomparisonDifferentModel
â”œâ”€â”€ Results_v2/hyperparameter_tuning/   â† trainingobjectiveï¼šadjustmentLearning Rate
â”œâ”€â”€ Results_v2/baseline_comparison/     â† trainingobjectiveï¼š and originalDatacomparison
â””â”€â”€ Results_v2/ablation_study/          â† trainingobjectiveï¼šablation experiment
```

**Key points**ï¼š
- âœ… SameDatasetï¼ˆ`prompt_engineering/copa_mezo_v1`ï¼‰can use  at multipleDifferenttrainingExperiment
- âœ… Each training experiment has its own purpose, results are classified according to training purpose
- âŒ If not classified, all results will be mixed in the `prompt_engineering` directory, and cannot be distinguishshedsh

---

## ğŸ“‹ Directorystructure

### **NewResults_v2structure**

```
Results_v2/
â””â”€â”€ {experiment_purpose}/           # ğŸ†• Experiment purpose classification (aligned with Data_v2)
    â””â”€â”€ {Model}/
        â””â”€â”€ {Task}_{Method}_{DataType}_{LR}/
            â””â”€â”€ {Timestamp}/
                â”œâ”€â”€ experiment_config.yaml  # ExperimentConfiguration
                â”œâ”€â”€ {lr}_train.out         # trainingoutput
                â”œâ”€â”€ {lr}_train.err         # Erroroutput
                â””â”€â”€ ...                    # Modelcheckpoint etc.
```

### **DirectoryDescription**

1. **experiment_purpose**: Experimentobjectiveclassification
   -  and Data_v2experiment_purpose for should
   - example such as ï¼š`prompt_engineering`, `temperature_study`, `model_comparison`

2. **Model**: ModelName
   - example such as ï¼š`meta-llama/Llama-3.2-1B`, `mistralai/Mistral-Nemo-Base-2407`

3. **Task_Method_DataType_LR**: Experiment identifier
   - Task: taskNameï¼ˆCopa, BOOLQ, CB etc.ï¼‰
   - Method: Training Methodï¼ˆzo, fo_full, fo_loraï¼‰
   - DataType: Dataclasstypeï¼ˆoriginal, synthetic etc.ï¼‰
   - LR: Learning Rateï¼ˆformat, e.g. `1_7` indicates 1e-7ï¼‰

4. **Timestamp**: timestamp (format: YYYYMMDD_HHMMSS)
   - Running same configuration multiple times will create different timestamp directories

---

## ğŸ¯ coreFeature

### **1. trainingExperimentobjectiveclassification**

Training Results according to **trainingExperimentobjective**classificationï¼ˆ and DataGenerateobjectiveindependentestablishï¼‰ï¼š

```yaml
# ConfigurationFile
experiment:
  purpose: "hyperparameter_tuning"  # ğŸ”´ trainingobjectiveï¼resultSave to : Results_v2/hyperparameter_tuning/

data:
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"
  #                        â†‘ DataGenerateobjectiveï¼ˆ and trainingobjectiveDifferentï¼‰
```

### **2. Must explicitly specify training purpose**

`experiment.purpose` must be explicitly specified, otherwise results will use default value `uncategorized`ï¼š

```yaml
# âœ… Recommended: Explicitly specify
experiment:
  purpose: "model_comparison"

# âš ï¸  If not specified, results will be saved to Results_v2/uncategorized/
```

**Recommended training experiment purpose categories**ï¼š
- `baseline_comparison` -  and baselinecomparison
- `model_comparison` - comparisonDifferentModel
- `hyperparameter_tuning` - HyperparametersTune
- `ablation_study` - ablation experiment
- `prompt_effectiveness` - TestpromptEffect
- `data_quality_impact` - TestDataqualityImpact
- `scaling_study` - Scalability research

### **3. CompletemetaDataTrace**

eachtrainingExperimentAutomaticSaveCompleteConfigurationï¼š

```yaml
# experiment_config.yaml
timestamp: "20251226_143000"
experiment_purpose: "prompt_engineering"
model: "meta-llama/Llama-3.2-1B"
task: "Copa"
method: "zo"
data:
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"
hyperparameters:
  learning_rate: 1e-6
  batch_size: 16
  steps: 20000
  seed: 0
training_info:
  env_vars: {...}
  command: "..."
  out_file: "..."
  err_file: "..."
```

---

## ğŸ“– useGuide

### **Scenario1ï¼šHyperparametersTuneï¼ˆuseSyntheticDataï¼‰**

```yaml
# training_config.yaml
experiment:
  purpose: "hyperparameter_tuning"  # ğŸ”´ trainingobjectiveï¼šTuneHyperparameters
  description: "usecopa_mezo_v1DataTestDifferentLearning Rate"

model: "meta-llama/Llama-3.2-1B"
task: "Copa"
method: "zo"

data:
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"  # ğŸ†• Directly specify path

hyperparameters:
  learning_rate: [1e-6, 5e-7, 2e-7, 1e-7]
  batch_size: 16
  steps: 20000
  seed: 0
```

**Runtraining**ï¼š
```bash
python automation/stage2_training/trainer.py training_config.yaml
```

**resultSave to **ï¼š
```
Results_v2/hyperparameter_tuning/meta-llama/Llama-3.2-1B/
                â†‘  according to trainingobjectiveclassificationï¼ˆnotYesDataGenerateobjectiveï¼‰
â”œâ”€â”€ Copa_zo_copa_mezo_v1_1_6/
â”‚   â””â”€â”€ 20251226_143000/
â”œâ”€â”€ Copa_zo_copa_mezo_v1_5_7/
â”‚   â””â”€â”€ 20251226_143000/
â”œâ”€â”€ Copa_zo_copa_mezo_v1_2_7/
â”‚   â””â”€â”€ 20251226_143000/
â””â”€â”€ Copa_zo_copa_mezo_v1_1_7/
    â””â”€â”€ 20251226_143000/
```

### **Scenario2ï¼šModelcomparisonï¼ˆusesameDataï¼‰**

```yaml
# training_config.yaml
experiment:
  purpose: "model_comparison"  # ğŸ”´ trainingobjectiveï¼šcomparisonDifferentModel
  description: " in copa_mezo_v1Data up comparisonLlama and Mistral"

model: "mistralai/Mistral-Nemo-Base-2407"  # ğŸ”§ TestDifferentModel
task: "Copa"
method: "zo"

data:
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"
  #                        â†‘ Datafromselfprompt_engineeringExperiment
  #                        â†‘ buttrainingobjectiveYesmodel_comparison

hyperparameters:
  learning_rate: 5e-7  # useKnownBestLearning Rate
  batch_size: 16
  steps: 20000
  seed: 0
```

**System as **ï¼š
- DataSourceï¼š`Data_v2/synthetic/prompt_engineering/...`
- trainingobjectiveï¼š`model_comparison`ï¼ˆ and DataGenerateobjectiveDifferentï¼‰
- resultSave to ï¼š`Results_v2/model_comparison/`

### **Scenario3ï¼šBaselinecomparisonï¼ˆoriginalData vs SyntheticDataï¼‰**

```yaml
# training_config.yaml
experiment:
  purpose: "baseline_comparison"  # ğŸ”´ trainingobjectiveï¼šcomparisonbaseline
  description: "comparisonoriginalData and SyntheticDatatrainingEffect"

model: "meta-llama/Llama-3.2-1B"
task: "Copa"
method: "zo"

data:
  path: "Data_v2/original/Copa"  # ğŸ”§ useoriginalDataas as baseline

hyperparameters:
  learning_rate: 5e-7  # use and SyntheticDatasameHyperparameters
  batch_size: 16
  steps: 20000
  seed: 0
```

**resultSave to **ï¼š
```
Results_v2/baseline_comparison/meta-llama/Llama-3.2-1B/Copa_zo_original_5_7/20251226_143000/
```

**comparisonAnalysis**ï¼š
```
SyntheticDataresultï¼šResults_v2/hyperparameter_tuning/.../Copa_zo_copa_mezo_v1_5_7/...
originalDataresultï¼šResults_v2/baseline_comparison/.../Copa_zo_original_5_7/...
â†‘ twoExperimentallSave in eachselfExperimentobjectivedirectory ï¼Œmethodeasycomparison
```

---

## ğŸ”§ ManageTool

### **list_results.py**

listoutandManageallTraining Resultsã€‚

#### **Viewsummary need **

```bash
python automation/stage2_training/list_results.py
```

**outputExample**ï¼š
```
================================================================================
Training Resultssummary need  - Results_v2
================================================================================

ğŸ“ Experimentobjective: prompt_engineering
   Experimentcount: 12
   â””â”€ meta-llama/Llama-3.2-1B: 12 Experiment

ğŸ“ Experimentobjective: temperature_study
   Experimentcount: 8
   â””â”€ meta-llama/Llama-3.2-1B: 8 Experiment

ğŸ“ Experimentobjective: baseline
   Experimentcount: 4
   â””â”€ meta-llama/Llama-3.2-1B: 4 Experiment

================================================================================
total: 3 Experimentobjective, 24 trainingExperiment
================================================================================
```

#### **ViewDetailedinformation**

```bash
# ViewallExperimentDetailedinformation
python automation/stage2_training/list_results.py --detail

# ViewspecialspecifyExperimentobjectiveDetailedinformation
python automation/stage2_training/list_results.py --detail --purpose prompt_engineering
```

**outputExample**ï¼š
```
================================================================================
Training ResultsDetails
================================================================================

ğŸ“ Experimentobjective: prompt_engineering
--------------------------------------------------------------------------------

  [1] Copa_zo_copa_mezo_v1_1_6
      Model: meta-llama/Llama-3.2-1B
      time: 20251226_143000
      Path: Results_v2/prompt_engineering/meta-llama/Llama-3.2-1B/Copa_zo_copa_mezo_v1_1_6/20251226_143000
      task: Copa
      method: zo
      Hyperparameters:
        - LR: 1e-06
        - BS: 16
        - Steps: 20000
        - Seed: 0
      Data: Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa

  [2] Copa_zo_copa_mezo_v1_5_7
      ...
```

---

## ğŸ”„ Data-result for shouldOffsystem

### **CompleteExperimentTracechain**

```
Stage 1ï¼šDataGenerate
Data_v2/synthetic/
â””â”€â”€ prompt_engineering/           # Experimentobjective
    â””â”€â”€ copa_mezo_v1/              # ExperimentID
        â”œâ”€â”€ Copa/                  # Dataset
        â”‚   â”œâ”€â”€ copa_train.jsonl
        â”‚   â”œâ”€â”€ copa_validation.jsonl
        â”‚   â””â”€â”€ copa_test.jsonl
        â””â”€â”€ experiment_metadata.json  # DataGeneration parameters

                    â¬‡

Stage 2ï¼šModel Training
Results_v2/
â””â”€â”€ prompt_engineering/           # ğŸ”— sameExperimentobjective
    â””â”€â”€ meta-llama/Llama-3.2-1B/
        â””â”€â”€ Copa_zo_copa_mezo_v1_1_6/
            â””â”€â”€ 20251226_143000/
                â””â”€â”€ experiment_config.yaml  # trainingParameter
```

### ** for shouldOffsystem**

| Dataset | Training Results |
|--------|----------|
| `Data_v2/synthetic/{purpose}/{exp_id}/{Dataset}` | `Results_v2/{purpose}/{Model}/{Task}_{Method}_{exp_id}_{LR}/{Timestamp}` |

**Key points**ï¼š
- `{purpose}`  in twosideMaintainConsistent
- `{exp_id}`  in Results Directoryname in bodycurrent
- pass`experiment_config.yaml` in `data.path`canTrace to sourceData

---

## ğŸ“Š best practices

### **1. trainingExperimentobjectiveNamingspecification**

**Recommended training experiment purpose categories**ï¼ˆStage 2ï¼‰ï¼š

- `baseline_comparison` -  and baselinecomparison
- `model_comparison` - ModelcomparisonExperiment
- `hyperparameter_tuning` - HyperparametersTune
- `ablation_study` - ablation experiment
- `prompt_effectiveness` - TestpromptEffect
- `data_quality_impact` - TestDataqualityImpact
- `scaling_study` - Scalability research
- `method_comparison` - Training Methodcomparisonï¼ˆMeZO vs LoRA vs Full FTï¼‰

**DataGenerateExperimentobjectiveclasscategory**ï¼ˆStage 1ï¼ŒonlyprovideReferenceï¼‰ï¼š

- `prompt_engineering` - PromptoptimizeExperiment
- `temperature_study` - TemperatureParameterResearch
- `data_quality_optimization` - Dataqualityoptimize
- `few_shot_study` - Few-shotExampleResearch

### **2. ConfigurationFileOrganize**

 according to **trainingExperimentobjective**OrganizeConfigurationFileï¼š

```
automation/configs/stage2/
â”œâ”€â”€ baseline_comparison/
â”‚   â”œâ”€â”€ copa_original.yaml
â”‚   â””â”€â”€ boolq_original.yaml
â”œâ”€â”€ model_comparison/
â”‚   â”œâ”€â”€ copa_llama_vs_mistral.yaml
â”‚   â””â”€â”€ copa_llama_1b_vs_3b.yaml
â”œâ”€â”€ hyperparameter_tuning/
â”‚   â”œâ”€â”€ copa_lr_sweep.yaml
â”‚   â””â”€â”€ copa_bs_sweep.yaml
â””â”€â”€ prompt_effectiveness/
    â”œâ”€â”€ copa_v1_vs_v2.yaml
    â””â”€â”€ copa_temp_comparison.yaml
```

**Note**ï¼šConfigurationFile according to trainingobjectiveclassificationï¼ŒnotYes according to Datasetclassification

### **3. ExperimentRecord**

eachtimesImportantExperiment back ï¼Œ in  for shouldExperimentobjectivedirectory Recordï¼š

```bash
#  in Results_v2/{trainingobjective}/README.md in Record
echo "## ExperimentRecord

### 2025-12-26: Learning RatescanExperiment
- trainingobjective: hyperparameter_tuning
- Dataset: Data_v2/synthetic/prompt_engineering/copa_mezo_v1/
- Model: Llama-3.2-1B
- Learning Rategrid: [1e-6, 5e-7, 2e-7, 1e-7]
- Bestresult: LR=5e-7, Acc=85.2%
- preparenote: 5e-7YesBestLearning Rateï¼Œ use  at  back continueExperiment
" >> Results_v2/hyperparameter_tuning/README.md
```

---

## âš ï¸ Notematteritem

### **1. Stage 1 and Stage 2ExperimentobjectiveYesindependentestablishï¼**

ğŸ”´ **mostImportantconcept**ï¼š

```
âŒ ErrorUnderstandï¼š
   Datafromself Data_v2/synthetic/prompt_engineering/...
   â†’ resultshouldSave to  Results_v2/prompt_engineering/

âœ… correctUnderstandï¼š
   Datafromself Data_v2/synthetic/prompt_engineering/...  â† DataGenerateobjective
   trainingobjectiveYes hyperparameter_tuning                    â† trainingExperimentobjective
   â†’ resultSave to  Results_v2/hyperparameter_tuning/
```

### **2. mustexplicitlyreferspecifytrainingExperimentobjective**

System**not will ** from DataPathAutomaticInferencetrainingExperimentobjectiveï¼š

```yaml
# âŒ Errorï¼šnohasreferspecifyexperiment.purpose
data:
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"
# â†’ result will Save to  Results_v2/uncategorized/

# âœ… correctï¼šexplicitlyreferspecifytrainingobjective
experiment:
  purpose: "hyperparameter_tuning"
data:
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"
# â†’ resultSave to  Results_v2/hyperparameter_tuning/
```

### **3. Oldformatcompatibility**

SystemstillsupportOld`data.type`formatï¼ŒbutRecommendeduseNew`data.path`ï¼š

```yaml
# âœ… Recommendedï¼ˆNewformatï¼‰
data:
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"

# âš ï¸  alreadydiscard use ï¼ˆOldformatï¼‰
data:
  type: "synthetic_mezo_gpt4o_v1"
```

### **3. timestampisolated**

sameConfigurationmultipletimesRun will CreateDifferenttimestampDirectoryï¼ŒavoidOverrideï¼š

```
Copa_zo_copa_mezo_v1_1_6/
â”œâ”€â”€ 20251226_143000/  # line1timesRun
â”œâ”€â”€ 20251226_153000/  # line2timesRun
â””â”€â”€ 20251227_093000/  # line3timesRun
```

---

## ğŸ‰ Summary

### **NewSystemAdvantage**

1. âœ… **Experimentobjectiveclassification**ï¼šresult according to ExperimentobjectiveAutomaticOrganize
2. âœ… **smart can Inference**ï¼š from DataPathAutomaticInferenceExperimentobjective
3. âœ… **CompleteTrace**ï¼šDataset â†” Training ResultsComplete for should
4. âœ… **metaData management**ï¼šAutomaticSaveallExperimentParameter
5. âœ… **ManageTool**ï¼šlist_results.pyQuickViewresult

### ** and OldSystemcomparison**

| Feature | OldSystem | NewSystem |
|------|--------|--------|
| resultOrganize | âŒ allresultmixed in thisup | âœ…  according to Experimentobjectiveclassification |
| ExperimentTrace | âŒ ManualRecord | âœ… AutomaticTrace to Dataset |
| ConfigurationManage | âš ï¸  PartialSave | âœ… CompleteSave |
| ViewTool | âŒ None | âœ… list_results.py |

---

**OnstartyoutrainingExperimentï¼** ğŸš€
