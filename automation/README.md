# Synthetic Data Automation Pipeline

**Version**: 2.1
**Last Updated**: 2026-01-01
**Status**: âœ… All P0 Bugs Fixed

Complete automation tools for synthetic data generation and training

---

## ðŸ“š Core Documentation

### **Recommended Reading Order for New Users**

1. **[SETUP_GUIDE.md](SETUP_GUIDE.md)** ðŸ”§ - Environment Setup Guide (Must-read for first-time users)
2. **[USER_GUIDE.md](USER_GUIDE.md)** ðŸ“– - User Manual (Complete guide)
3. **[COMPLETE_PIPELINE_SIMULATION.md](COMPLETE_PIPELINE_SIMULATION.md)** ðŸ“‹ - Complete Pipeline Example
4. **[BATCH_GUIDE.md](BATCH_GUIDE.md)** ðŸ“¦ - Batch Solution 3++ Complete Guide
5. **[TOOLS_REFERENCE.md](TOOLS_REFERENCE.md)** ðŸ› ï¸ - All Tools Reference Manual

### **Other Documentation**

- **[BUG_FIXES_SUMMARY.md](BUG_FIXES_SUMMARY.md)** - Bug Fixes Summary (v2.1)
- **[stage2_training/RESULTS_MANAGEMENT.md](stage2_training/RESULTS_MANAGEMENT.md)** - Training Results Management System

## Directory Structure

```
automation/
â”œâ”€â”€ stage1_generation/              # Stage 1: Synthetic Data Generation
â”‚   â”œâ”€â”€ generator.py                # Data generator (using Batch Solution 3++)
â”‚   â”œâ”€â”€ experiment_manager_batch.py # Batch experiment manager (core)
â”‚   â”œâ”€â”€ batch_tools/                # Batch management tools
â”‚   â”‚   â”œâ”€â”€ list_batches.py         # List all batches
â”‚   â”‚   â”œâ”€â”€ list_batch_experiments.py  # View experiments in batch
â”‚   â”‚   â”œâ”€â”€ list_shared_experiments.py # View physical data
â”‚   â”‚   â”œâ”€â”€ compare_experiments.py     # Compare experiment parameters
â”‚   â”‚   â”œâ”€â”€ list_data_paths.py â­   # Find data paths (new)
â”‚   â”‚   â””â”€â”€ resolve_data_path.py â­ # Path conversion (new)
â”‚   â”œâ”€â”€ tools/                      # Manual review tools
â”‚   â”‚   â”œâ”€â”€ review_top20.py
â”‚   â”‚   â”œâ”€â”€ annotate_samples.py
â”‚   â”‚   â””â”€â”€ publish_dataset.py (optional)
â”‚   â””â”€â”€ WORKFLOW.md                 # Complete workflow documentation
â”‚
â”œâ”€â”€ stage2_training/                # Stage 2: Training
â”‚   â”œâ”€â”€ trainer.py                  # Training automation (LoRA bugs fixed)
â”‚   â”œâ”€â”€ list_results.py             # View training results
â”‚   â””â”€â”€ RESULTS_MANAGEMENT.md       # Results_v2 system documentation
â”‚
â”œâ”€â”€ configs/                        # Configuration files
â”‚   â”œâ”€â”€ examples/                   # Configuration examples
â”‚   â”‚   â”œâ”€â”€ stage1_full_example_copa.yaml
â”‚   â”‚   â””â”€â”€ stage2_example_training.yaml
â”‚   â”œâ”€â”€ stage1/                     # Stage 1 configs (user)
â”‚   â””â”€â”€ stage2/                     # Stage 2 configs (user)
â”‚
â”œâ”€â”€ config.py                       # Unified path configuration (new)
â”œâ”€â”€ fix_hardcoded_paths.py          # Path fixing script (for maintenance)
â”‚
â”œâ”€â”€ SETUP_GUIDE.md                  # Environment setup guide (new)
â”œâ”€â”€ USER_GUIDE.md                   # User manual
â”œâ”€â”€ BATCH_GUIDE.md                  # Batch Solution 3++ guide
â”œâ”€â”€ TOOLS_REFERENCE.md              # All tools reference (new)
â”œâ”€â”€ COMPLETE_PIPELINE_SIMULATION.md # Complete pipeline example
â”œâ”€â”€ BUG_FIXES_SUMMARY.md            # Bug fixes summary (new)
â””â”€â”€ README.md                       # This document
```

## Quick Start

### Stage 1: Generate Synthetic Data (Using Batch Solution 3++)

#### Step 1: Create Configuration File

```bash
cp configs/examples/stage1_full_example_copa.yaml configs/stage1/my_copa_config.yaml
```

Edit the configuration file, key modifications:

```yaml
# Experiment Management (Batch Solution 3++)
experiment:
  batch_id: "batch_20241229_temperature"  # Batch ID (optional)
  purpose: "temperature_study"            # Experiment purpose
  description: "Study the impact of temperature parameter on synthetic data quality"

# Dataset Configuration
dataset:
  task_name: "copa"
  input_path: "Data/original/Copa/copa_train.jsonl"

# Generation Configuration
generation:
  model: "gpt-4o"
  temperature: 0.7           # These parameters will calculate fingerprint
  top_p: 1.0                 # For automatic deduplication
  rephrase_prompt: |
    Your rephrasing prompt...

# Validation Configuration
validation:
  model: "gpt-4o"
  temperature: 0.0
  validation_prompt: |
    Your validation prompt...
  few_shot_examples:
    - ...
```

#### Step 2: Generate Data

```bash
cd automation/stage1_generation

# Run generator
python generator.py --config ../configs/stage1/my_copa_config.yaml

# The system will automatically:
# 1. Calculate parameter fingerprint (based on model, temperature, top_p, etc.)
# 2. Check if data with the same fingerprint exists in _shared/Copa/
# 3. If exists â†’ Reuse physical data, create batch symbolic link
# 4. If not â†’ Generate new data to _shared/, create batch symbolic link
```

Output directory structure:

```
Data_v2/synthetic/
â”œâ”€â”€ _shared/                                # Physical data storage
â”‚   â””â”€â”€ Copa/
â”‚       â””â”€â”€ temp07_topp10_gpt4o/           # Actual data files
â”‚           â”œâ”€â”€ copa_train.jsonl
â”‚           â”œâ”€â”€ copa_validation.jsonl
â”‚           â”œâ”€â”€ copa_test.jsonl
â”‚           â”œâ”€â”€ .fingerprint               # Parameter fingerprint
â”‚           â””â”€â”€ experiment_metadata.json   # Complete metadata
â”‚
â””â”€â”€ batch_20241229_temperature/            # Batch view (symbolic links)
    â””â”€â”€ Copa/
        â””â”€â”€ temp07_topp10_gpt4o -> ../../_shared/Copa/temp07_topp10_gpt4o/
```

#### Step 3: View and Manage Experiments

```bash
# List all batches
python batch_tools/list_batches.py --verbose

# View experiments in batch
python batch_tools/list_batch_experiments.py batch_20241229_temperature --verbose

# View all experiments in physical storage (check if parameter configuration already generated)
python batch_tools/list_shared_experiments.py --dataset Copa --verbose

# Compare parameters of two experiments
python batch_tools/compare_experiments.py \
    --shared Copa/temp07_topp10_gpt4o \
    --shared Copa/temp09_topp10_gpt4o
```

### Stage 2: Training

#### Step 1: Create Training Configuration

```bash
cp configs/examples/stage2_example_training.yaml configs/stage2/my_training.yaml
```

Edit configuration file:

```yaml
experiment:
  purpose: "temperature_study"  # Training experiment purpose (independent from data generation purpose)
  description: "Evaluate training effectiveness of data generated with different temperature parameters"

model: "meta-llama/Llama-3.2-1B"
task: "Copa"
method: "zo"  # zo, fo_full, fo_lora

data:
  # Using data generated with Batch solution
  path: "Data_v2/synthetic/_shared/Copa/temp07_topp10_gpt4o"

hyperparameters:
  learning_rate: [1e-6, 5e-7]
  batch_size: 16
  steps: 20000
  seed: 0

cuda_devices: "0"
```

#### Step 2: Execute Training

```bash
cd automation/stage2_training

# Preview (without execution)
python trainer.py --config ../configs/stage2/my_training.yaml --dry-run

# Execute training
python trainer.py --config ../configs/stage2/my_training.yaml
```

Results saved in:

```
Results_v2/
â””â”€â”€ temperature_study/              # Organized by training experiment purpose
    â””â”€â”€ Llama-3.2-1B/
        â””â”€â”€ Copa_zo_temp07_topp10_1e-6/
            â””â”€â”€ 20241229_120000/
                â”œâ”€â”€ train.out
                â”œâ”€â”€ train.err
                â””â”€â”€ experiment_config.yaml
```

---

## ðŸ†• v2.1 New Features

### 1. Unified Path Configuration System

```python
# automation/config.py - Automatically detect project root
from config import PROJECT_ROOT, DATA_V2_DIR, RESULTS_V2_DIR
```

**Advantages**:
- âœ… Support running project from any location
- âœ… Automatic path detection
- âœ… Environment variable override support

### 2. Data Path Finding Tool

```bash
# Quickly find training data paths
python stage1_generation/batch_tools/list_data_paths.py --dataset Copa --format yaml
```

**Purpose**: Quickly find data paths when writing training configurations

### 3. Path Conversion Tool

```bash
# Batch path â†” Shared path conversion
python stage1_generation/batch_tools/resolve_data_path.py "Data_v2/..."
```

**Purpose**: Understand the relationship between batch and shared paths

### 4. Bug Fixes

âœ… **Fixed 5 P0-Level Bugs**:
1. LoRA training script name error
2. LoRA environment variable error
3. Hardcoded absolute paths
4. batch_tools path hierarchy error
5. list_data_paths API error

See details: [BUG_FIXES_SUMMARY.md](BUG_FIXES_SUMMARY.md)

---

## Core Advantages of Batch Solution 3++

### 1. Automatic Parameter Deduplication

```bash
# Experiment 1: batch_20241229_temperature
generation:
  temperature: 0.7
  top_p: 1.0
# â†’ Generated to _shared/Copa/temp07_topp10_gpt4o/

# Experiment 2: batch_20241230_topp (reusing same parameters)
generation:
  temperature: 0.7  # Same
  top_p: 1.0        # Same
# â†’ Detected same fingerprint, directly reuse _shared/Copa/temp07_topp10_gpt4o/
# â†’ Create symbolic link to batch_20241230_topp/Copa/temp07_topp10_gpt4o/
```

### 2. Flexible Experiment Organization

```
Data_v2/synthetic/
â”œâ”€â”€ _shared/                        # Physical storage (shared by all experiments)
â”‚   â””â”€â”€ Copa/
â”‚       â”œâ”€â”€ temp05_topp10_gpt4o/
â”‚       â”œâ”€â”€ temp07_topp10_gpt4o/
â”‚       â””â”€â”€ temp09_topp10_gpt4o/
â”‚
â”œâ”€â”€ batch_20241229_temperature/     # Batch 1: Temperature experiment
â”‚   â””â”€â”€ Copa/
â”‚       â”œâ”€â”€ temp05_topp10_gpt4o -> ../../_shared/Copa/temp05_topp10_gpt4o/
â”‚       â”œâ”€â”€ temp07_topp10_gpt4o -> ../../_shared/Copa/temp07_topp10_gpt4o/
â”‚       â””â”€â”€ temp09_topp10_gpt4o -> ../../_shared/Copa/temp09_topp10_gpt4o/
â”‚
â””â”€â”€ batch_20241230_model/           # Batch 2: Model comparison
    â””â”€â”€ Copa/
        â””â”€â”€ temp07_topp10_gpt4o -> ../../_shared/Copa/temp07_topp10_gpt4o/  (Reused!)
```

### 3. Complete Metadata Tracking

Each physical experiment directory contains:
- `.fingerprint`: Parameter fingerprint (12-char MD5)
- `experiment_metadata.json`: Complete configuration, creation time, original batch, etc.

## Common Scenarios

### Scenario 1: Temperature Parameter Experiment

```bash
# Create 3 config files, only modifying temperature
for temp in 0.5 0.7 0.9; do
  sed "s/temperature:.*/temperature: $temp/" base_config.yaml > copa_temp${temp/.}.yaml
  python generator.py --config copa_temp${temp/.}.yaml
done

# System automatically creates 3 independent physical experiments
# _shared/Copa/temp05_topp10_gpt4o/
# _shared/Copa/temp07_topp10_gpt4o/
# _shared/Copa/temp09_topp10_gpt4o/
```

### Scenario 2: Cross-batch Data Reuse

```bash
# Batch1 already generated temp07 data
# Batch2 needs same parameters â†’ Automatically reuse, no regeneration

python batch_tools/list_batch_experiments.py batch_20241230_model --verbose
# Output:
# âš¡ Data reuse: Yes (original batch: batch_20241229_temperature)
```

## Compatibility with Training Scripts

### âœ… Direct Use of Data_v2 Paths (Recommended)

trainer.py can directly use `Data_v2/` paths, **no publish needed**:

```yaml
# Training configuration
data:
  # Recommended: Use batch path (more intuitive)
  path: "Data_v2/synthetic/batch_20241229_temperature/Copa/temp07_topp10_gpt4o/Copa"

  # Or use shared path (also works)
  # path: "Data_v2/synthetic/_shared/Copa/temp07_topp10_gpt4o/Copa"
```

### Optional: Publish to Data/ (For Legacy Script Compatibility Only)

```bash
python stage1_generation/tools/publish_dataset.py \
    --source Data_v2/synthetic/_shared/Copa/temp07_topp10_gpt4o \
    --dataset Copa \
    --target Data/rejection_sampling/0_data
```

**Note**: Only for compatibility with legacy training script structure, not needed for new projects.

## Migrating Old Data

To migrate synthetic data and training results from old projects to the new system, see: **[MIGRATION_GUIDE.md](MIGRATION_GUIDE.md)**

Information needed:
- Dataset name and location
- Generation model and method (recommended)
- Training experiment purpose
- Training configuration and hyperparameters

## FAQ

### Q1: Will the Batch solution waste storage space?

No. Data with the same parameter configuration is stored only once in `_shared/`, different batches reference it through symbolic links.

### Q2: How to check if data for a certain parameter configuration has been generated?

```bash
python batch_tools/list_shared_experiments.py --dataset Copa --verbose
```

### Q3: Can I delete old batches?

Yes. Deleting batch directories won't delete physical data (in `_shared/`). Only manually delete physical data when it's not referenced by any batch.

### Q4: What's the difference between training experiment purpose and data generation purpose?

- **Data generation purpose** (`experiment.purpose`): Why generate this data (e.g., `temperature_study`)
- **Training experiment purpose**: Why train with this data (e.g., `baseline_comparison`)

They are independent, training results are organized by training experiment purpose in `Results_v2/`.

## Dependencies

```bash
# Python dependencies
pip install pyyaml openai tqdm

# Environment variables
export OPENAI_API_KEY="your-api-key"
```

## Contributing

Issues and Pull Requests are welcome!

## License

MIT
