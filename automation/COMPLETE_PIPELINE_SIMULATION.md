# Complete Pipeline Interaction Flow Simulation

This document simulates the complete interaction process between users and the system in the entire synthetic data generation pipeline, covering **all 5 datasets** and **two generation strategies**.

## ğŸ“š Table of Contents

### Part 1: Two-Stage Mode (Detailed Complete Workflow)
- [Scenario 1: Copa Dataset (Two-Stage Mode)](#scenario-1-copa-dataset-two-stage-mode)
- [Scenario 2: BOOLQ Dataset (Two-Stage Mode)](#scenario-2-boolq-dataset-two-stage-mode)
- [Scenario 3: CB Dataset (Two-Stage Mode)](#scenario-3-cb-dataset-two-stage-mode)
- [Scenario 4: RTE Dataset (Two-Stage Mode)](#scenario-4-rte-dataset-two-stage-mode)
- [Scenario 5: ArcC Dataset (Two-Stage Mode)](#scenario-5-arcc-dataset-two-stage-mode)

### Part 2: Direct-All Mode (Parameter Study)
- [Scenario 6: Copa Dataset (Direct-All Mode)](#scenario-6-copa-dataset-direct-all-mode)
- [Scenario 7: BOOLQ Dataset (Direct-All Mode)](#scenario-7-boolq-dataset-direct-all-mode)
- [Scenario 8: CB Dataset (Direct-All Mode)](#scenario-8-cb-dataset-direct-all-mode)
- [Scenario 9: RTE Dataset (Direct-All Mode)](#scenario-9-rte-dataset-direct-all-mode)
- [Scenario 10: ArcC Dataset (Direct-All Mode)](#scenario-10-arcc-dataset-direct-all-mode)

### Appendix
- [Dataset Comparison Table](#dataset-comparison-table)
- [Key Points Summary](#key-points-summary)

---

# Part 1: Two-Stage Mode (Detailed Complete Workflow)

## Scenario 1: Copa Dataset (Two-Stage Mode)

### Step 0: Prepare Configuration File

```bash
$ cd /home/ubuntu/LLM-inference/jikai-project/Synthetic_Data_for_ZO/automation

# Copy configuration template
$ cp configs/examples/stage1_full_example_copa.yaml configs/stage1/my_copa.yaml

# Edit configuration file
$ vim configs/stage1/my_copa.yaml
```

**Configuration Content**:
```yaml
experiment:
  batch_id: "batch_20241230_copa_baseline"
  purpose: "copa_baseline"
  description: "Copa dataset baseline experiment"

task_name: "Copa"
training_method: "mezo"

dataset:
  task_name: "copa"
  dataset_name: "Copa"
  input_path: "Data/original/Copa/copa_train.jsonl"
  original_dir: "Data/original/Copa"
  fields:
    - "premise"
    - "choice1"
    - "choice2"
    - "question"
    - "label"

generation:
  # Rewriter API configuration
  api_key: "sk-eWSYPo0CvhRYgcJs55B0C3F00aC74f6e95F47c1f4772292c"
  base_url: "https://api2.aigcbest.top/v1"
  timeout: 120

  strategy: "two_stage"  # Default, exploratory experiment
  model: "gpt-4o"
  temperature: 0.9
  top_p: 1.0
  field_to_rephrase: "premise"

  rephrase_prompt: |
    You are tasked with rephrasing the given premise...
    {{REPHRASE_FEWSHOT}}
    ... (complete prompt) ...

validation:
  # Judge API configuration
  api_key: "sk-eWSYPo0CvhRYgcJs55B0C3F00aC74f6e95F47c1f4772292c"
  base_url: "https://api2.aigcbest.top/v1"
  timeout: 120

  model: "gpt-4o"
  temperature: 0.0

  validation_prompt: |
    Task: Verify if the rephrased premise maintains consistency...
    {{VALIDATION_FEWSHOT}}
    ... (complete prompt) ...

  few_shot_examples: []  # Initially empty, auto-generated after checkpoint 2A
```

### Step 1: Generate Scripts

```bash
$ cd automation/stage1_generation
$ python generator.py ../configs/stage1/my_copa.yaml
```

**System Output**:
```
================================================================================
Synthetic Data Generation Script Auto-Generator
================================================================================
Generation Strategy: two_stage
Experiment Purpose: copa_baseline
Experiment Description: Copa dataset baseline experiment
Task: Copa
Training Method: mezo
Generation Model: gpt-4o
Validation Model: gpt-4o
================================================================================

================================================================================
ğŸ”§ Batch Experiment Management
================================================================================
Batch ID: batch_20241230_copa_baseline
Dataset: Copa
Parameter Fingerprint: a7b3c2d91f45
Semantic Name: temp09_topp10_gpt4o
================================================================================

ğŸ” Searching for fingerprint a7b3c2d91f45 in _shared/Copa/...
âœ“ No matching experiment found, will create new experiment

ğŸ“‚ Creating New Experiment
   Physical Storage: _shared/Copa/temp09_topp10_gpt4o
   Batch View: batch_20241230_copa_baseline/Copa/temp09_topp10_gpt4o

Output Directory: Data_v2/synthetic/_shared/Copa/temp09_topp10_gpt4o
Parameter Fingerprint: a7b3c2d91f45
Dataset Directory: Data_v2/synthetic/_shared/Copa/temp09_topp10_gpt4o/Copa

Generating rephrase scripts...
  âœ“ rephrase_all.py
  âœ“ rephrase_top20.py
  âœ“ rephrase_rest.py

Generating validation scripts...
  âœ“ validate.py

Saving configuration...
âœ“ Config Copy: generation_config.yaml
âœ“ Experiment Metadata: experiment_metadata.json
âœ“ README: README.md

================================================================================
Generation Complete!
================================================================================

Script Location: Data_v2/synthetic/_shared/Copa/temp09_topp10_gpt4o/scripts

Usage (two_stage mode):
  1. Run generation: python .../rephrase_top20.py
  2. Run validation: python .../validate.py

Note: API configuration loaded from config file, no need to set environment variables
```

### Step 2: Checkpoint 1 - Generate First 20 Samples

```bash
$ cd Data_v2/synthetic/_shared/Copa/temp09_topp10_gpt4o/scripts
$ python rephrase_top20.py
```

**System Output**:
```
Loaded 400 original data samples
Output file: ../Copa/copa_train_top20.jsonl

Processing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [02:15<00:00]

Complete! Output: ../Copa/copa_train_top20.jsonl
```

### Step 3: Checkpoint 1 - Manual Review of First 20 Samples

```bash
$ cp ../../../../automation/stage1_generation/tools/review_top20.py .
$ python review_top20.py
```

**System Output**:
```
Loading data...
  Original: /home/ubuntu/.../Data/original/Copa/copa_train.jsonl
  Rephrased: ../Copa/copa_train_top20.jsonl

================================================================================
First 20 Samples Comparison - Please carefully review original vs rephrased data
================================================================================

ã€Sample 1ã€‘
  Original premise:  My body cast a shadow over the grass.
  Rephrased premise:  A shadow appeared on the grass beside me.
  Choice1: The sun was rising.
  Choice2: The grass was cut.
  Question: cause

ã€Sample 2ã€‘
  Original premise:  The woman tolerated her friend's difficult behavior.
  Rephrased premise:  The woman was patient with her friend's challenging attitude.
  Choice1: The woman knew her friend was going through a hard time.
  Choice2: The woman felt that her friend took advantage of her kindness.
  Question: cause

... (Samples 3-20) ...

================================================================================

Enter the sample numbers (1-20) that are unqualified, separated by commas
Example: 3,7,15  means samples 3, 7, 15 are unqualified
If all are qualified, press Enter directly

Unqualified sample numbers:
```

**User Input**:
```
3,7,8
```

**System Continues Output**:
```
Statistics:
  Qualified samples: 17
  Unqualified samples: 3

Performing rejection sampling...
  Sample 3: Using original data (unqualified)
  Sample 7: Using original data (unqualified)
  Sample 8: Using original data (unqualified)

âœ“ Rejection sampling complete
  - 17 rephrased data samples (good quality)
  - 3 original data samples (replaced rejected samples)

Saving results...
âœ“ Saved: ../Copa/copa_train_top20.jsonl

Generating few-shot examples...
âœ“ Generated 17 few-shot examples

Injecting few-shot into rephrase_rest.py...
âœ“ Few-shot examples injected to: rephrase_rest.py
  Backup saved: rephrase_rest.py.backup

================================================================================
âœ… Checkpoint 1 Complete!
================================================================================

Next step:
  Run: python rephrase_rest.py
```

### Step 4: Generate Remaining Data

```bash
$ python rephrase_rest.py
```

**System Output**:
```
Loaded 400 original data samples
Output file: ../Copa/copa_train_rest.jsonl

Processing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 380/380 [45:30<00:00]

Complete! Output: ../Copa/copa_train_rest.jsonl
```

### Step 5: Merge Data

```bash
$ cat ../Copa/copa_train_top20.jsonl ../Copa/copa_train_rest.jsonl > ../Copa/copa_train.jsonl
$ wc -l ../Copa/copa_train.jsonl
```

**System Output**:
```
400 ../Copa/copa_train.jsonl
```

### Step 6: Checkpoint 2A - Process Samples 21-40

```bash
$ cp ../../../../automation/stage1_generation/tools/annotate_samples.py .
$ python annotate_samples.py --range 21-40
```

**System Output**ï¼š
```
Loading data...
  åŸå§‹: /home/ubuntu/.../Data/original/Copa/copa_train.jsonl
  æ”¹å†™: ../Copa/copa_train.jsonl

================================================================================
æ ·æœ¬ 21-40 å¯¹æ¯” - è¯·ä»”ç»†æŸ¥çœ‹åŸå§‹æ•°æ®ä¸æ”¹å†™æ•°æ®
================================================================================

ã€Sample 21ã€‘
  Original premise:  The girl received a trophy.
  Rephrased premise:  The young lady was awarded a trophy for her achievement.
  choice1: She won a spelling bee.
  choice2: She made a new friend.
  question: cause
  label: 0

... (æ ·æœ¬22-40) ...

================================================================================

è¯·è¾“å…¥ä¸åˆæ ¼æ ·æœ¬çš„åºå·ï¼ˆ21-40ï¼‰ï¼Œå¤šä¸ªåºå·ç”¨é€—å·åˆ†éš”
ç¤ºä¾‹: 23,27,35  è¡¨ç¤ºè¿™å‡ ä¸ªæ ·æœ¬ä¸åˆæ ¼
If all are qualified, press Enter directly

Unqualified sample numbers:
```

**User Input**ï¼š
```
23,35
```

**System Continues Output**ï¼š
```
Statistics:
  Qualified samples: 18
  Unqualified samples: 2

ã€Checkpoint 2A: ç¬¬21-40ä¸ªæ ·æœ¬å¤„ç†ã€‘

Performing rejection sampling...
  Sample23: Using original data (unqualified)
  Sample35: Using original data (unqualified)

âœ“ Rejection sampling complete: ../validation_checkpoints/samples_21_40_validated.jsonl

ç”Ÿæˆvalidation few-shot examples...
âœ“ Generated18ä¸ªvalidation few-shot examples
âœ“ Validation few-shotå·²ä¿å­˜: ../validation_checkpoints/validation_fewshot.json

âœ“ æ ‡æ³¨è®°å½•å·²ä¿å­˜: ../validation_checkpoints/samples_21_40_annotation.json

================================================================================
âœ… æ ·æœ¬ 21-40 å¤„ç†å®Œæˆï¼
================================================================================

âœ… Checkpoint 2A Completeæ€»ç»“:
  1. Rejection sampling: 18/20 ä¸ªæ ·æœ¬ä¿ç•™æ”¹å†™
  2. Validation few-shot: ç”Ÿæˆäº† 18 ä¸ªexamples

Next step:
  Run: python annotate_samples.py --range 41-80
```

### Step7: Checkpoint 2B - å¤„ç†ç¬¬41-80ä¸ªæ ·æœ¬

```bash
$ python annotate_samples.py --range 41-80
```

**System Output**ï¼ˆæ˜¾ç¤ºæ ·æœ¬41-80åï¼‰ï¼š
```
è¯·è¾“å…¥ä¸åˆæ ¼æ ·æœ¬çš„åºå·ï¼ˆ41-80ï¼‰ï¼Œå¤šä¸ªåºå·ç”¨é€—å·åˆ†éš”
ç¤ºä¾‹: 43,47,55,72  è¡¨ç¤ºè¿™å‡ ä¸ªæ ·æœ¬ä¸åˆæ ¼
If all are qualified, press Enter directly

Unqualified sample numbers:
```

**User Input**ï¼š
```
43,47,72
```

**System Continues Output**ï¼š
```
Statistics:
  Qualified samples: 37
  Unqualified samples: 3

ã€Checkpoint 2B: ç¬¬41-80ä¸ªæ ·æœ¬å¤„ç†ã€‘

Performing rejection sampling...
  Sample43: Using original data (unqualified)
  Sample47: Using original data (unqualified)
  Sample72: Using original data (unqualified)

âœ“ Rejection sampling complete: ../validation_checkpoints/samples_41_80_validated.jsonl

ç”Ÿæˆtest_set...
âœ“ Generated40ä¸ªtestæ ·æœ¬
  Ground Truthæ ‡æ³¨Statistics:
  - same (åˆæ ¼): 37
  - not the same (ä¸åˆæ ¼): 3
âœ“ Test setå·²ä¿å­˜: ../validation_checkpoints/validation_test_set.json
  ç”¨é€”: æµ‹è¯•AI judge validation promptçš„å‡†ç¡®ç‡

âœ“ æ ‡æ³¨è®°å½•å·²ä¿å­˜: ../validation_checkpoints/samples_41_80_annotation.json

================================================================================
âœ… æ ·æœ¬ 41-80 å¤„ç†å®Œæˆï¼
================================================================================

âœ… Checkpoint 2B Completeæ€»ç»“:
  1. Rejection sampling: 37/40 ä¸ªæ ·æœ¬ä¿ç•™æ”¹å†™
  2. Test set: ç”Ÿæˆäº† 40 ä¸ªæ ‡æ³¨æ ·æœ¬
  3. Ground Truth: same=37, not the same=3

Next step:
  ä½¿ç”¨test_setæµ‹è¯•validation promptå‡†ç¡®ç‡
  Run: python generate_validation_test.py
```

### Step8: æµ‹è¯•AI Judgeå‡†ç¡®ç‡

```bash
$ cp ../../../../automation/stage1_generation/tools/generate_validation_test.py .
$ python generate_validation_test.py
```

**System Output**ï¼š
```
åŠ è½½test set...
  æ–‡ä»¶: ../validation_checkpoints/validation_test_set.json
  Sampleæ•°: 40

åŠ è½½validationé…ç½®...
  æ¨¡å‹: gpt-4o
  Temperature: 0.0

å¼€å§‹æµ‹è¯•AI judge...

æµ‹è¯•æ ·æœ¬ 1/40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [05:30<00:00]

================================================================================
ğŸ“Š æµ‹è¯•ç»“æœ
================================================================================
æ€»æµ‹è¯•æ ·æœ¬: 40
AIåˆ¤æ–­ä¸º same: 38
AIåˆ¤æ–­ä¸º not the same: 2

ä¸Ground Truthå¯¹æ¯”:
  âœ“ åˆ¤æ–­æ­£ç¡®: 39
  âœ— åˆ¤æ–­é”™è¯¯: 1

å‡†ç¡®ç‡: 97.5%

================================================================================
âœ… æµ‹è¯•é€šè¿‡ï¼å‡†ç¡®ç‡ â‰¥ 95%
================================================================================

å¯ä»¥ç»§ç»­æ‰§è¡ŒCheckpoint 3ï¼ˆè‡ªåŠ¨éªŒè¯å‰©ä½™æ•°æ®ï¼‰
```

### Step9: Checkpoint 3 - è‡ªåŠ¨éªŒè¯å‰©ä½™æ•°æ®

```bash
$ python validate.py
```

**System Output**ï¼š
```
åŠ è½½è®­ç»ƒæ•°æ®...
  æ–‡ä»¶: ../Copa/copa_train.jsonl
  æ€»æ ·æœ¬æ•°: 400

å·²å¤„ç†æ ·æœ¬ï¼ˆCheckpoint 1å’Œ2ï¼‰: 80
å¾…éªŒè¯æ ·æœ¬: 320 (æ ·æœ¬81-400)

åŠ è½½validationé…ç½®...
  æ¨¡å‹: gpt-4o
  Temperature: 0.0
  Few-shot examples: 18ä¸ª

å¼€å§‹è‡ªåŠ¨éªŒè¯...

éªŒè¯è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [38:45<00:00]

================================================================================
ğŸ“Š éªŒè¯ç»“æœç»Ÿè®¡
================================================================================
æ€»éªŒè¯æ ·æœ¬: 320
åˆ¤æ–­ä¸º same: 307 (95.9%)
åˆ¤æ–­ä¸º not the same: 13 (4.1%)

Performing rejection sampling...
  âœ“ ä¿ç•™æ”¹å†™: 307 æ¡
  âœ— æ›¿æ¢ä¸ºåŸå§‹: 13 æ¡

ä¿å­˜æœ€ç»ˆæ•°æ®...
âœ“ Saved: ../Copa/copa_train_final.jsonl

å¤åˆ¶validationå’Œtesté›†...
âœ“ å·²å¤åˆ¶: ../Copa/copa_validation.jsonl
âœ“ å·²å¤åˆ¶: ../Copa/copa_test.jsonl

================================================================================
âœ… æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼
================================================================================

æœ€ç»ˆæ•°æ®é›†:
  è®­ç»ƒé›†: ../Copa/copa_train_final.jsonl (400æ¡)
    - æ”¹å†™æ•°æ®: 359æ¡ (89.8%)
    - åŸå§‹æ•°æ®: 41æ¡ (10.2%)
  éªŒè¯é›†: ../Copa/copa_validation.jsonl
  æµ‹è¯•é›†: ../Copa/copa_test.jsonl

æ•°æ®é›†è·¯å¾„: Data_v2/synthetic/_shared/Copa/temp09_topp10_gpt4o/Copa/
å¯ç›´æ¥ç”¨äºMeZOè®­ç»ƒï¼
```

---

## Scenario2: BOOLQ Dataset (Two-Stage Mode)

### Dataset Characteristics

- **Task Type**: å¸ƒå°”é—®ç­”ï¼ˆYes/Noï¼‰
- **Field to Rephrase**: `passage`ï¼ˆæ®µè½ï¼‰
- **Other Fields**: `question`ï¼ˆé—®é¢˜ï¼‰ã€`label`ï¼ˆ0=No, 1=Yesï¼‰

### Key Configuration Modifications

```yaml
experiment:
  purpose: "boolq_baseline"

task_name: "BOOLQ"
training_method: "mezo"

dataset:
  task_name: "boolq"
  dataset_name: "BOOLQ"
  input_path: "Data/original/BOOLQ/boolq_train.jsonl"
  original_dir: "Data/original/BOOLQ"
  fields:
    - "passage"
    - "question"
    - "label"

generation:
  field_to_rephrase: "passage"  # BOOLQæ”¹å†™passageå­—æ®µ

  rephrase_prompt: |
    You are tasked with rephrasing the given passage...
    {{REPHRASE_FEWSHOT}}

    **Original passage**: "{passage}"
    **Question**: "{question}"
    **Answer**: {"Yes" if label == 1 else "No"}

    **Directly output only one rephrased passage**:
```

### The workflow is the same as Copa

æ‰§è¡Œæ­¥éª¤1-10ä¸Copaç›¸åŒï¼Œåªæ˜¯å­—æ®µåä»`premise`å˜ä¸º`passage`ã€‚

### Sample Comparison Examples (BOOLQ specific)

```
ã€Sample 1ã€‘
  Original passage:  The Supreme Court of the United States is the highest federal court...
  Rephrased passage:  As the highest federal court in the United States, the Supreme Court...
  question: is the supreme court the highest court in the united states
  label: 1 (Yes)

ã€Sample 2ã€‘
  Original passage:  A mule is the offspring of a male donkey and a female horse...
  Rephrased passage:  The hybrid animal known as a mule results from breeding a male donkey...
  question: can a mule reproduce
  label: 0 (No)
```

---

## Scenario3: CB Dataset (Two-Stage Mode)

### Dataset Characteristics

- **Task Type**: è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰
- **Field to Rephrase**: `hypothesis`ï¼ˆå‡è®¾ï¼‰
- **Other Fields**: `premise`ï¼ˆå‰æï¼‰ã€`label`ï¼ˆ0=entailment, 1=contradiction, 2=neutralï¼‰

### Key Configuration Modifications

```yaml
experiment:
  purpose: "cb_baseline"

task_name: "CB"
training_method: "mezo"

dataset:
  task_name: "cb"
  dataset_name: "CB"
  input_path: "Data/original/CB/cb_train.jsonl"
  original_dir: "Data/original/CB"
  fields:
    - "premise"
    - "hypothesis"
    - "label"

generation:
  field_to_rephrase: "hypothesis"  # CBæ”¹å†™hypothesiså­—æ®µ

  rephrase_prompt: |
    You are tasked with rephrasing the given hypothesis...
    {{REPHRASE_FEWSHOT}}

    **Premise**: "{premise}"
    **Original hypothesis**: "{hypothesis}"
    **Label**: {["entailment", "contradiction", "neutral"][label]}

    **Directly output only one rephrased hypothesis**:
```

### Sample Comparison Examples (CB specific)

```
ã€Sample 1ã€‘
  Original premise:  It was a complex language. Not written down but handed down.
  Original hypothesis:  the language was written down
  Rephrased hypothesis:  the language existed in written form
  label: 1 (contradiction)

ã€Sample 2ã€‘
  Original premise:  Valence the void great quietness is there.
  Original hypothesis:  Great quietness is in the void.
  Rephrased hypothesis:  The void contains significant quietness.
  label: 0 (entailment)
```

---

## Scenario4: RTE Dataset (Two-Stage Mode)

### Dataset Characteristics

- **Task Type**: è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆRecognizing Textual Entailmentï¼‰
- **Field to Rephrase**: `premise`ï¼ˆå‰æï¼‰
- **Other Fields**: `hypothesis`ï¼ˆå‡è®¾ï¼‰ã€`label`ï¼ˆ0=entailment, 1=not_entailmentï¼‰
- **Data Example**:
  ```json
  {"premise": "No Weapons of Mass Destruction Found in Iraq Yet.",
   "hypothesis": "Weapons of Mass Destruction Found in Iraq.",
   "label": 1}
  ```

### Key Configuration Modifications

```yaml
experiment:
  batch_id: "batch_20241230_rte_baseline"
  purpose: "rte_baseline"
  description: "RTEæ•°æ®é›†åŸºçº¿å®éªŒ"

task_name: "RTE"
training_method: "mezo"

dataset:
  task_name: "rte"
  dataset_name: "RTE"
  input_path: "Data/original/RTE/rte_train.jsonl"
  original_dir: "Data/original/RTE"
  fields:
    - "premise"
    - "hypothesis"
    - "label"

generation:
  strategy: "two_stage"
  model: "gpt-4o"
  temperature: 0.9
  top_p: 1.0
  field_to_rephrase: "premise"  # RTEæ”¹å†™premiseå­—æ®µ

  rephrase_prompt: |
    You are tasked with rephrasing the given premise for a textual entailment task.
    {{REPHRASE_FEWSHOT}}

    **Original premise**: "{premise}"
    **Hypothesis**: "{hypothesis}"
    **Label**: {["entailment", "not_entailment"][label]}

    **Directly output only one rephrased premise**:

validation:
  model: "gpt-4o"
  temperature: 0.0

  validation_prompt: |
    Task: Verify if the rephrased premise maintains semantic consistency...
    {{VALIDATION_FEWSHOT}}

    **Original premise**: "{original_premise}"
    **Rephrased premise**: "{rephrased_premise}"
    **Hypothesis**: "{hypothesis}"

    Is the rephrased premise semantically equivalent? Answer "same" or "not the same":

  few_shot_examples: []
```

### Sample Comparison Examples (RTE specific)

```
ã€Sample 1ã€‘
  Original premise:  No Weapons of Mass Destruction Found in Iraq Yet.
  Rephrased premise:  Weapons of mass destruction have not been discovered in Iraq so far.
  hypothesis: Weapons of Mass Destruction Found in Iraq.
  label: 1 (not_entailment)

ã€Sample 2ã€‘
  Original premise:  The European Union says the Greek Cypriot community will be admitted to the EU.
  Rephrased premise:  According to the European Union, Greek Cypriots will join the EU.
  hypothesis: Cyprus was divided into two parts in 1974.
  label: 1 (not_entailment)

ã€Sample 3ã€‘
  Original premise:  Russia's Mikhail Khodorkovsky, the former head of oil giant Yukos, was convicted.
  Rephrased premise:  Mikhail Khodorkovsky, who previously led the major oil company Yukos, was found guilty.
  hypothesis: Mikhail Khodorkovsky was Russia's richest man.
  label: 1 (not_entailment)
```

### Complete Execution Workflow

æµç¨‹ä¸Copaå®Œå…¨ç›¸åŒï¼ˆæ­¥éª¤1-10ï¼‰ï¼Œåªéœ€ï¼š
1. å‡†å¤‡RTEé…ç½®æ–‡ä»¶
2. æ‰§è¡Œ `python generator.py ../configs/stage1/my_rte.yaml`
3. æŒ‰ç…§Checkpoint 1â†’Checkpoint 2Aâ†’Checkpoint 2Bâ†’Checkpoint 3ä¾æ¬¡æ‰§è¡Œ

---

## Scenario5: ArcC Dataset (Two-Stage Mode)

### Dataset Characteristics

- **Task Type**: å¤šé€‰é¢˜ï¼ˆç§‘å­¦æ¨ç†ï¼‰
- **Field to Rephrase**: `question`ï¼ˆé—®é¢˜ï¼‰
- **Other Fields**: `choices`ï¼ˆé€‰é¡¹ï¼‰ã€`answerKey`ï¼ˆç­”æ¡ˆï¼‰
- **Data Example**:
  ```json
  {"id": "Mercury_SC_415702",
   "question": "George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?",
   "choices": {
     "text": ["dry palms", "wet palms", "palms covered with oil", "palms covered with lotion"],
     "label": ["A", "B", "C", "D"]
   },
   "answerKey": "A"}
  ```

### Key Configuration Modifications

```yaml
experiment:
  batch_id: "batch_20241230_arcc_baseline"
  purpose: "arcc_baseline"
  description: "ARC-Challengeæ•°æ®é›†åŸºçº¿å®éªŒ"

task_name: "ArcC"
training_method: "mezo"

dataset:
  task_name: "arc_challenge"
  dataset_name: "ArcC"
  input_path: "Data/original/ArcC_Cloze/ARC-Challenge_train.jsonl"
  original_dir: "Data/original/ArcC_Cloze"
  fields:
    - "question"
    - "choices"
    - "answerKey"

generation:
  strategy: "two_stage"
  model: "gpt-4o"
  temperature: 0.9
  top_p: 1.0
  field_to_rephrase: "question"  # ArcCæ”¹å†™questionå­—æ®µ

  rephrase_prompt: |
    You are tasked with rephrasing multiple-choice science questions.
    {{REPHRASE_FEWSHOT}}

    **Original question**: "{question}"
    **Choices**: {', '.join([f"{label}: {text}" for label, text in zip(choices['label'], choices['text'])])}
    **Correct answer**: {answerKey}

    **Directly output only one rephrased question**:

validation:
  model: "gpt-4o"
  temperature: 0.0

  validation_prompt: |
    Task: Verify if the rephrased question maintains the same meaning...
    {{VALIDATION_FEWSHOT}}

    **Original question**: "{original_question}"
    **Rephrased question**: "{rephrased_question}"
    **Choices**: {choices}
    **Correct answer**: {answerKey}

    Is the rephrased question semantically equivalent? Answer "same" or "not the same":

  few_shot_examples: []
```

### Sample Comparison Examples (ArcC specific)

```
ã€Sample 1ã€‘
  Original question:  George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?
  Rephrased question:  To rapidly warm his hands through rubbing, which type of skin surface should George use to generate maximum heat?
  choices: A: dry palms, B: wet palms, C: palms covered with oil, D: palms covered with lotion
  answerKey: A

ã€Sample 2ã€‘
  Original question:  A student wants to look under a heavy rock. Which simple machine would be BEST to use to lift the rock?
  Rephrased question:  What simple machine would be most effective for a student attempting to lift a large, heavy rock?
  choices: A: Wheel and axle, B: Lever, C: Inclined plane, D: Screw
  answerKey: B

ã€Sample 3ã€‘
  Original question:  Which of these do scientists most likely do when studying the interaction of animals in their natural habitat?
  Rephrased question:  When observing animals in their natural environment, which activity would scientists typically perform?
  choices: A: design a mathematical model, B: perform a controlled experiment, C: collect data, D: formulate a hypothesis
  answerKey: C
```

### Complete Execution Workflow

æµç¨‹ä¸Copaå®Œå…¨ç›¸åŒï¼ˆæ­¥éª¤1-10ï¼‰ï¼Œåªæ˜¯æ”¹å†™å­—æ®µä¸º`question`ã€‚

---

# Part 2: Direct-Allæ¨¡å¼ï¼ˆå‚æ•°ç ”ç©¶ï¼‰

## Scenario6: Copa Dataset (Direct-All Mode)

### Use Case

å·²ç»é€šè¿‡ç¬¬ä¸€æ¬¡two-stageç”Ÿæˆè·å¾—äº†å¯ç”¨çš„promptå’Œfew-shot examplesï¼Œç°åœ¨æƒ³è¦å¿«é€Ÿæ¢ç©¶ä¸åŒtemperatureå‚æ•°ï¼ˆ0.5, 0.7, 0.9ï¼‰å¯¹åˆæˆæ•°æ®è´¨é‡çš„å½±å“ã€‚

### Step1: å‡†å¤‡Direct-Allé…ç½®

```bash
$ cd automation/configs/stage1
$ cp ../examples/stage1_direct_all_copa.yaml temperature_05.yaml
$ vim temperature_05.yaml
```

**é…ç½®å†…å®¹**ï¼š
```yaml
experiment:
  batch_id: "batch_20241230_temperature_study"
  purpose: "temperature_comparison"
  description: "æ¯”è¾ƒtemperature=0.5/0.7/0.9å¯¹Copaåˆæˆæ•°æ®è´¨é‡çš„å½±å“"

task_name: "Copa"
training_method: "mezo"

dataset:
  task_name: "copa"
  dataset_name: "Copa"
  input_path: "Data/original/Copa/copa_train.jsonl"
  original_dir: "Data/original/Copa"
  fields:
    - "premise"
    - "choice1"
    - "choice2"
    - "question"
    - "label"

generation:
  # Rewriter APIé…ç½®
  api_key: "sk-eWSYPo0CvhRYgcJs55B0C3F00aC74f6e95F47c1f4772292c"
  base_url: "https://api2.aigcbest.top/v1"
  timeout: 120

  strategy: "direct_all"  # ğŸ”¥ ç›´æ¥å…¨é‡ç”Ÿæˆ
  model: "gpt-4o"
  temperature: 0.5  # ğŸ”¬ å‚æ•°å˜é‡
  top_p: 1.0
  field_to_rephrase: "premise"

  # âš ï¸ å¿…é¡»åŒ…å«å®Œæ•´çš„few-shotï¼ˆä»ç¬¬ä¸€æ¬¡two-stageç”Ÿæˆä¸­è·å¾—ï¼‰
  rephrase_prompt: |
    You are tasked with rephrasing the given premise...

    ### Few-shot Examples:
    Original premise: "My body cast a shadow over the grass."
    Rephrased premise: "A shadow appeared on the grass beside me."

    Original premise: "The woman tolerated her friend's difficult behavior."
    Rephrased premise: "The woman was patient with her friend's challenging attitude."

    ... (å®Œæ•´çš„17 few-shot examples) ...

    ### Your Task:
    **Original premise**: "{premise}"
    **Choice 1**: "{choice1}"
    **Choice 2**: "{choice2}"
    **Question**: "{question}"
    **Correct answer**: "{choice1 if label == 0 else choice2}"

    **Directly output only one rephrased premise**:

# âš ï¸ direct_allæ¨¡å¼ä¸éœ€è¦validationé…ç½®
```

### Step2: ç”Ÿæˆ3ä¸ªä¸åŒtemperatureçš„é…ç½®

```bash
# Temperature 0.5
$ cp temperature_05.yaml temperature_05.yaml

# Temperature 0.7
$ sed 's/temperature: 0.5/temperature: 0.7/' temperature_05.yaml > temperature_07.yaml

# Temperature 0.9
$ sed 's/temperature: 0.5/temperature: 0.9/' temperature_05.yaml > temperature_09.yaml
```

### Step3: ç”Ÿæˆå¹¶è¿è¡Œï¼ˆTemperature 0.5ï¼‰

```bash
$ cd ../stage1_generation
$ python generator.py ../configs/stage1/temperature_05.yaml

$ cd Data_v2/synthetic/_shared/Copa/temp05_topp10_gpt4o/scripts
$ python rephrase_all.py
```

**System Output**ï¼š
```
Loaded 400 original data samples
Output file: ../Copa/copa_train.jsonl

Processing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [48:30<00:00]

Complete! Output: ../Copa/copa_train.jsonl
```

### Step4: é‡å¤ç”Ÿæˆå…¶ä»–temperature

```bash
# Temperature 0.7
$ cd ../../stage1_generation
$ python generator.py ../configs/stage1/temperature_07.yaml
$ cd Data_v2/synthetic/_shared/Copa/temp07_topp10_gpt4o/scripts
$ python rephrase_all.py

# Temperature 0.9
$ cd ../../stage1_generation
$ python generator.py ../configs/stage1/temperature_09.yaml
$ cd Data_v2/synthetic/_shared/Copa/temp09_topp10_gpt4o/scripts
$ python rephrase_all.py
```

### Batchç³»ç»Ÿè‡ªåŠ¨ç®¡ç†

```
Data_v2/synthetic/
â”œâ”€â”€ _shared/                              # ç‰©ç†å­˜å‚¨
â”‚   â””â”€â”€ Copa/
â”‚       â”œâ”€â”€ temp05_topp10_gpt4o/         # Temperature 0.5
â”‚       â”‚   â””â”€â”€ Copa/copa_train.jsonl
â”‚       â”œâ”€â”€ temp07_topp10_gpt4o/         # Temperature 0.7
â”‚       â”‚   â””â”€â”€ Copa/copa_train.jsonl
â”‚       â””â”€â”€ temp09_topp10_gpt4o/         # Temperature 0.9
â”‚           â””â”€â”€ Copa/copa_train.jsonl
â”‚
â””â”€â”€ batch_20241230_temperature_study/    # Batchè§†å›¾ï¼ˆç¬¦å·é“¾æ¥ï¼‰
    â””â”€â”€ Copa/
        â”œâ”€â”€ temp05_topp10_gpt4o -> ../../_shared/Copa/temp05_topp10_gpt4o/
        â”œâ”€â”€ temp07_topp10_gpt4o -> ../../_shared/Copa/temp07_topp10_gpt4o/
        â””â”€â”€ temp09_topp10_gpt4o -> ../../_shared/Copa/temp09_topp10_gpt4o/
```

---

## Scenario7: BOOLQ Dataset (Direct-All Mode)

### é…ç½®æ–‡ä»¶å…³é”®å·®å¼‚

```yaml
experiment:
  batch_id: "batch_20241230_boolq_topp_study"
  purpose: "boolq_topp_comparison"
  description: "æ¯”è¾ƒtop_p=0.8/0.9/1.0å¯¹BOOLQåˆæˆæ•°æ®è´¨é‡çš„å½±å“"

task_name: "BOOLQ"

dataset:
  task_name: "boolq"
  dataset_name: "BOOLQ"
  input_path: "Data/original/BOOLQ/boolq_train.jsonl"
  fields:
    - "passage"
    - "question"
    - "label"

generation:
  strategy: "direct_all"
  temperature: 0.9  # å›ºå®š
  top_p: 0.8  # ğŸ”¬ ç ”ç©¶å˜é‡
  field_to_rephrase: "passage"

  rephrase_prompt: |
    You are tasked with rephrasing the given passage...

    ### Few-shot Examples:
    Original passage: "The Supreme Court of the United States is..."
    Rephrased passage: "As the highest federal court in..."

    ... (17ä¸ªå®Œæ•´examples) ...

    **Original passage**: "{passage}"
    **Question**: "{question}"
    **Answer**: {"Yes" if label == 1 else "No"}

    **Directly output only one rephrased passage**:
```

### æ‰§è¡Œæµç¨‹

```bash
# ç”Ÿæˆtop_p=0.8çš„é…ç½®
$ python generator.py ../configs/stage1/boolq_topp08.yaml
$ cd Data_v2/synthetic/_shared/BOOLQ/temp09_topp08_gpt4o/scripts
$ python rephrase_all.py

# ç”Ÿæˆtop_p=0.9çš„é…ç½®
$ python generator.py ../configs/stage1/boolq_topp09.yaml
$ cd Data_v2/synthetic/_shared/BOOLQ/temp09_topp09_gpt4o/scripts
$ python rephrase_all.py

# ç”Ÿæˆtop_p=1.0çš„é…ç½®
$ python generator.py ../configs/stage1/boolq_topp10.yaml
$ cd Data_v2/synthetic/_shared/BOOLQ/temp09_topp10_gpt4o/scripts
$ python rephrase_all.py
```

---

## Scenario8: CB Dataset (Direct-All Mode)

### é…ç½®æ–‡ä»¶å…³é”®å·®å¼‚

```yaml
experiment:
  batch_id: "batch_20241230_cb_model_study"
  purpose: "cb_model_comparison"
  description: "æ¯”è¾ƒgpt-4o vs gpt-4o-miniå¯¹CBåˆæˆæ•°æ®è´¨é‡çš„å½±å“"

task_name: "CB"

dataset:
  task_name: "cb"
  dataset_name: "CB"
  input_path: "Data/original/CB/cb_train.jsonl"
  fields:
    - "premise"
    - "hypothesis"
    - "label"

generation:
  strategy: "direct_all"
  model: "gpt-4o"  # ğŸ”¬ ç ”ç©¶å˜é‡ï¼ˆå¯æ”¹ä¸ºgpt-4o-miniï¼‰
  temperature: 0.9
  top_p: 1.0
  field_to_rephrase: "hypothesis"

  rephrase_prompt: |
    You are tasked with rephrasing the given hypothesis...

    ### Few-shot Examples:
    Premise: "It was a complex language. Not written down but handed down."
    Original hypothesis: "the language was written down"
    Rephrased hypothesis: "the language existed in written form"

    ... (17ä¸ªå®Œæ•´examples) ...

    **Premise**: "{premise}"
    **Original hypothesis**: "{hypothesis}"
    **Label**: {["entailment", "contradiction", "neutral"][label]}

    **Directly output only one rephrased hypothesis**:
```

### æ‰§è¡Œæµç¨‹

```bash
# GPT-4o
$ python generator.py ../configs/stage1/cb_gpt4o.yaml
$ cd Data_v2/synthetic/_shared/CB/temp09_topp10_gpt4o/scripts
$ python rephrase_all.py

# GPT-4o-mini
$ python generator.py ../configs/stage1/cb_gpt4o_mini.yaml
$ cd Data_v2/synthetic/_shared/CB/temp09_topp10_gpt4omini/scripts
$ python rephrase_all.py
```

---

## Scenario9: RTE Dataset (Direct-All Mode)

### é…ç½®æ–‡ä»¶å…³é”®å·®å¼‚

```yaml
experiment:
  batch_id: "batch_20241230_rte_temp_study"
  purpose: "rte_temperature_comparison"
  description: "æ¯”è¾ƒtemperature=0.5/0.7/0.9å¯¹RTEåˆæˆæ•°æ®è´¨é‡çš„å½±å“"

task_name: "RTE"

dataset:
  task_name: "rte"
  dataset_name: "RTE"
  input_path: "Data/original/RTE/rte_train.jsonl"
  fields:
    - "premise"
    - "hypothesis"
    - "label"

generation:
  strategy: "direct_all"
  model: "gpt-4o"
  temperature: 0.5  # ğŸ”¬ ç ”ç©¶å˜é‡
  top_p: 1.0
  field_to_rephrase: "premise"

  rephrase_prompt: |
    You are tasked with rephrasing the given premise for textual entailment...

    ### Few-shot Examples:
    Original premise: "No Weapons of Mass Destruction Found in Iraq Yet."
    Rephrased premise: "Weapons of mass destruction have not been discovered in Iraq so far."

    ... (17ä¸ªå®Œæ•´examples) ...

    **Original premise**: "{premise}"
    **Hypothesis**: "{hypothesis}"
    **Label**: {["entailment", "not_entailment"][label]}

    **Directly output only one rephrased premise**:
```

### æ‰§è¡Œæµç¨‹

```bash
# Temperature 0.5
$ python generator.py ../configs/stage1/rte_temp05.yaml
$ cd Data_v2/synthetic/_shared/RTE/temp05_topp10_gpt4o/scripts
$ python rephrase_all.py

# Temperature 0.7
$ python generator.py ../configs/stage1/rte_temp07.yaml
$ cd Data_v2/synthetic/_shared/RTE/temp07_topp10_gpt4o/scripts
$ python rephrase_all.py

# Temperature 0.9
$ python generator.py ../configs/stage1/rte_temp09.yaml
$ cd Data_v2/synthetic/_shared/RTE/temp09_topp10_gpt4o/scripts
$ python rephrase_all.py
```

---

## Scenario10: ArcC Dataset (Direct-All Mode)

### é…ç½®æ–‡ä»¶å…³é”®å·®å¼‚

```yaml
experiment:
  batch_id: "batch_20241230_arcc_temp_study"
  purpose: "arcc_temperature_comparison"
  description: "æ¯”è¾ƒtemperature=0.5/0.7/0.9å¯¹ArcCåˆæˆæ•°æ®è´¨é‡çš„å½±å“"

task_name: "ArcC"

dataset:
  task_name: "arc_challenge"
  dataset_name: "ArcC"
  input_path: "Data/original/ArcC_Cloze/ARC-Challenge_train.jsonl"
  fields:
    - "question"
    - "choices"
    - "answerKey"

generation:
  strategy: "direct_all"
  model: "gpt-4o"
  temperature: 0.5  # ğŸ”¬ ç ”ç©¶å˜é‡
  top_p: 1.0
  field_to_rephrase: "question"

  rephrase_prompt: |
    You are tasked with rephrasing multiple-choice science questions...

    ### Few-shot Examples:
    Original question: "George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?"
    Rephrased question: "To rapidly warm his hands through rubbing, which type of skin surface should George use to generate maximum heat?"

    ... (17ä¸ªå®Œæ•´examples) ...

    **Original question**: "{question}"
    **Choices**: {', '.join([f"{label}: {text}" for label, text in zip(choices['label'], choices['text'])])}
    **Correct answer**: {answerKey}

    **Directly output only one rephrased question**:
```

### æ‰§è¡Œæµç¨‹

```bash
# Temperature 0.5
$ python generator.py ../configs/stage1/arcc_temp05.yaml
$ cd Data_v2/synthetic/_shared/ArcC/temp05_topp10_gpt4o/scripts
$ python rephrase_all.py

# Temperature 0.7
$ python generator.py ../configs/stage1/arcc_temp07.yaml
$ cd Data_v2/synthetic/_shared/ArcC/temp07_topp10_gpt4o/scripts
$ python rephrase_all.py

# Temperature 0.9
$ python generator.py ../configs/stage1/arcc_temp09.yaml
$ cd Data_v2/synthetic/_shared/ArcC/temp09_topp10_gpt4o/scripts
$ python rephrase_all.py
```

---

# é™„å½•

## æ•°æ®é›†å¯¹æ¯”è¡¨

| åœºæ™¯ | æ•°æ®é›† | æ¨¡å¼ | æ”¹å†™å­—æ®µ | å…¶ä»–å­—æ®µ | æ–­ç‚¹æ•° | æ€»è€—æ—¶ï¼ˆä¼°ç®—ï¼‰ |
|------|--------|------|----------|----------|--------|---------------|
| åœºæ™¯1 | Copa | Two-Stage | premise | choice1, choice2, question, label | 3ä¸ª | ~90åˆ†é’Ÿ |
| åœºæ™¯2 | BOOLQ | Two-Stage | passage | question, label | 3ä¸ª | ~90åˆ†é’Ÿ |
| åœºæ™¯3 | CB | Two-Stage | hypothesis | premise, label | 3ä¸ª | ~90åˆ†é’Ÿ |
| åœºæ™¯4 | RTE | Two-Stage | premise | hypothesis, label | 3ä¸ª | ~90åˆ†é’Ÿ |
| åœºæ™¯5 | ArcC | Two-Stage | question | choices, answerKey | 3ä¸ª | ~90åˆ†é’Ÿ |
| åœºæ™¯6 | Copa | Direct-All | premise | choice1, choice2, question, label | 0ä¸ª | ~50åˆ†é’Ÿ |
| åœºæ™¯7 | BOOLQ | Direct-All | passage | question, label | 0ä¸ª | ~50åˆ†é’Ÿ |
| åœºæ™¯8 | CB | Direct-All | hypothesis | premise, label | 0ä¸ª | ~50åˆ†é’Ÿ |
| åœºæ™¯9 | RTE | Direct-All | premise | hypothesis, label | 0ä¸ª | ~50åˆ†é’Ÿ |
| åœºæ™¯10 | ArcC | Direct-All | question | choices, answerKey | 0ä¸ª | ~50åˆ†é’Ÿ |

## äººå·¥å‚ä¸æ—¶é—´å¯¹æ¯”

### Two-Stageæ¨¡å¼ï¼ˆåœºæ™¯1-5ï¼‰:
- **Checkpoint 1å®¡æ ¸**: æµè§ˆ20ä¸ªæ ·æœ¬ + è¾“å…¥åºå· â‰ˆ **1åˆ†é’Ÿ**
- **Checkpoint 2Aå®¡æ ¸**: æµè§ˆ20ä¸ªæ ·æœ¬ + è¾“å…¥åºå· â‰ˆ **1åˆ†é’Ÿ**
- **Checkpoint 2Bå®¡æ ¸**: æµè§ˆ40ä¸ªæ ·æœ¬ + è¾“å…¥åºå· â‰ˆ **2åˆ†é’Ÿ**
- **æ€»è®¡äººå·¥æ—¶é—´**: ~**4åˆ†é’Ÿ**

### Direct-Allæ¨¡å¼ï¼ˆåœºæ™¯6-10ï¼‰:
- **æ— éœ€äººå·¥å‚ä¸** âœ…
- å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œé€‚åˆå‚æ•°ç ”ç©¶

## å…³é”®è¦ç‚¹æ€»ç»“

### 1. æ‰¹é‡è¾“å…¥æ¨¡å¼
- ç”¨æˆ·åªéœ€è¾“å…¥ä¸åˆæ ¼åºå·ï¼ˆå¦‚ï¼š`3,7,12`ï¼‰ï¼Œæ— éœ€é€ä¸ªç¡®è®¤
- å¤§å¹…å‡å°‘äººå·¥äº¤äº’æ—¶é—´

### 2. è‡ªåŠ¨Rejection Sampling
- ç³»ç»Ÿè‡ªåŠ¨æ›¿æ¢ä¸åˆæ ¼æ ·æœ¬ä¸ºåŸå§‹æ•°æ®
- æ‰€æœ‰3ä¸ªæ–­ç‚¹ï¼ˆ1-20, 21-40, 41-80ï¼‰éƒ½æ‰§è¡Œrejection sampling

### 3. è‡ªåŠ¨Few-shotç”Ÿæˆ
- Checkpoint 1ï¼šä»17ä¸ªåˆæ ¼æ ·æœ¬ç”Ÿæˆrephrase few-shot
- Checkpoint 2Aï¼šä»18ä¸ªåˆæ ¼æ ·æœ¬ç”Ÿæˆvalidation few-shot

### 4. è‡ªåŠ¨æ ‡æ³¨
- æ‰€æœ‰same/not the sameæ ‡æ³¨ç”±ç³»ç»Ÿè‡ªåŠ¨å®Œæˆ
- ç”Ÿæˆtest_setç”¨äºæµ‹è¯•AI judgeå‡†ç¡®ç‡

### 5. å¤šæ•°æ®é›†é›¶ä»£ç æ”¯æŒ
- 5ä¸ªæ•°æ®é›†ï¼ˆCopa, BOOLQ, CB, RTE, ArcCï¼‰
- åªéœ€ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„å­—æ®µåå’Œprompt
- æ— éœ€ä¿®æ”¹ä»»ä½•ä»£ç 

### 6. å‚æ•°å»é‡ï¼ˆBatchæ–¹æ¡ˆ3++ï¼‰
- è‡ªåŠ¨æ£€æµ‹ç›¸åŒå‚æ•°é…ç½®
- ç‰©ç†å­˜å‚¨åœ¨`_shared/`ï¼Œé¿å…é‡å¤ç”Ÿæˆ
- Batchè§†å›¾é€šè¿‡ç¬¦å·é“¾æ¥ç»„ç»‡å®éªŒ

### 7. ä¸¤ç§ç”Ÿæˆç­–ç•¥
- **Two-Stage**: æ¢ç´¢æ€§å®éªŒï¼Œéœ€è¦ç¡®å®špromptå’Œfew-shot
- **Direct-All**: promptå·²ç¡®å®šï¼Œå¿«é€Ÿå‚æ•°ç ”ç©¶

## ä½¿ç”¨å»ºè®®

1. **é¦–æ¬¡å®éªŒ**ï¼šä½¿ç”¨Two-Stageæ¨¡å¼ç¡®å®šæœ€ä½³promptå’Œfew-shot examples
2. **å‚æ•°ç ”ç©¶**ï¼šä»Two-Stageè·å¾—few-shotåï¼Œä½¿ç”¨Direct-Allæ¨¡å¼å¿«é€Ÿç”Ÿæˆä¸åŒå‚æ•°é…ç½®çš„æ•°æ®
3. **äººå·¥å®¡æ ¸**ï¼šè®¤çœŸå®¡æ ¸å‰80ä¸ªæ ·æœ¬ï¼Œç¡®ä¿AI judgeå‡†ç¡®ç‡â‰¥95%
4. **Batchç®¡ç†**ï¼šä½¿ç”¨`list_batches.py`ç­‰å·¥å…·æŸ¥çœ‹å’Œç®¡ç†å®éªŒ
5. **æ•°æ®å¤ç”¨**ï¼šå–„ç”¨Batchç³»ç»Ÿçš„å‚æ•°å»é‡åŠŸèƒ½ï¼Œé¿å…é‡å¤ç”Ÿæˆ
