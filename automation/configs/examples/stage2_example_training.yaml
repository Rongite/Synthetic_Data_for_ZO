# TrainingPipelineConfigurationFileExample

# ========== Experiment Management ==========
experiment:
  # ðŸ”´ Important: This is the experiment purpose for "Training", independent from "Data Generation" experiment purpose!
  # Training experiment purpose examples:
  #   - model_comparison: Compare different models' performance
  #   - hyperparameter_tuning: Adjust hyperparameters
  #   - baseline_comparison: Comparison with baseline
  #   - ablation_study: Ablation study
  #   - prompt_effectiveness: Test prompt effectiveness
  purpose: "model_comparison"  # Training experiment purpose (basis for results classification)
  description: "Compare different models' performance on Copa synthetic data"

# Model Configuration
model: "meta-llama/Llama-3.2-1B"  # or "mistralai/Mistral-Nemo-Base-2407"

# Task
task: "Copa"  # Copa, CB, RTE, BOOLQ, ArcC_Cloze, ArcC_MC

# Training Method
method: "zo"  # zo (MeZO), fo_full (Full Fine-tune), fo_lora (LoRA)

# Data Configuration
data:
  # ðŸ†• Recommended format: Directly specify data path (will automatically infer experiment_purpose)
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"

  # Old format (deprecated but still supported):
  # type: "original"  # Data type
  #   - "original": Original data
  #   - "synthetic_{method}_{model}_{version}": Synthetic data
  #   - "mixed_{method}_{model}_{version}": Mixed data

# Hyperparameter Configuration (Supports single value or list for grid search)
hyperparameters:
  learning_rate:  # Learning rate (can be single value or list)
    - 1e-6
    - 5e-7
    - 2e-7
    - 1e-7

  batch_size: 16  # Batch Size

  steps: 20000  # Training steps

  seed: 0  # Random seed (can be single value or list)

  # MeZO Specific parameters
  zo_eps: 1e-3  # MeZO epsilon parameter

  # LoRA Specific parameters (Only used when method=fo_lora)
  lora_rank: 8
  lora_alpha: 16

# CUDA Device Configuration
cuda_devices: "0"  # Can be "0", "0,1", "0,1,2,3", etc.

# Run Options
wait_for_completion: false  # Whether to wait for training completion (false means run in background)
continue_on_error: true  # Whether to continue executing remaining experiments on error
