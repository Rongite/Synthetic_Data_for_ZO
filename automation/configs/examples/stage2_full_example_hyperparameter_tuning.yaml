# å®Œæ•´çš„é˜¶æ®µ2è®­ç»ƒé…ç½®ç¤ºä¾‹ - è¶…å‚æ•°è°ƒä¼˜å®éªŒ
# ä½¿ç”¨é˜¶æ®µ1ç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡ŒMeZOè®­ç»ƒ

# ========== å®éªŒç®¡ç† ==========
experiment:
  # ğŸ”´ é‡è¦ï¼šé˜¶æ®µ1å’Œé˜¶æ®µ2çš„å®éªŒç›®çš„æ˜¯ç‹¬ç«‹çš„ï¼
  #
  # ã€é˜¶æ®µ1 - æ•°æ®ç”Ÿæˆçš„å®éªŒç›®çš„ã€‘ï¼š
  #   Data_v2/synthetic/prompt_engineering/copa_mezo_v1/
  #   â†‘ purpose = "prompt_engineering"ï¼ˆæµ‹è¯•ä¸åŒpromptå¯¹æ•°æ®è´¨é‡çš„å½±å“ï¼‰
  #
  # ã€é˜¶æ®µ2 - è®­ç»ƒçš„å®éªŒç›®çš„ã€‘ï¼š
  #   purpose = "hyperparameter_tuning"ï¼ˆæµ‹è¯•ä¸åŒå­¦ä¹ ç‡å¯¹è®­ç»ƒæ•ˆæœçš„å½±å“ï¼‰
  #   â†‘ è¿™ä¸ªå€¼å†³å®šResultsä¿å­˜ä½ç½®
  #
  # ã€ç»“æœä¿å­˜ä½ç½®ã€‘ï¼š
  #   Results_v2/hyperparameter_tuning/  â† æŒ‰"è®­ç»ƒç›®çš„"åˆ†ç±»ï¼Œä¸æ˜¯æ•°æ®ç”Ÿæˆç›®çš„ï¼
  #
  # ã€ä¸ºä»€ä¹ˆè¦åˆ†å¼€ã€‘ï¼š
  #   - åŒä¸€ä¸ªæ•°æ®é›†å¯èƒ½ç”¨äºå¤šç§ä¸åŒçš„è®­ç»ƒå®éªŒ
  #   - ä¾‹å¦‚ï¼šç”¨prompt_engineeringæ•°æ®æµ‹è¯•model_comparison
  #   - å¦‚æœä¸åˆ†å¼€ï¼Œæ‰€æœ‰ç»“æœéƒ½ä¼šæ··åœ¨prompt_engineeringç›®å½•ä¸‹
  #
  purpose: "hyperparameter_tuning"  # è®­ç»ƒå®éªŒç›®çš„ï¼ˆå†³å®šResultsåˆ†ç±»ï¼‰
  description: "ä½¿ç”¨copa_mezo_v1åˆæˆæ•°æ®æµ‹è¯•ä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ"

# ========== æ¨¡å‹é…ç½® ==========
model: "meta-llama/Llama-3.2-1B"

# ========== ä»»åŠ¡ ==========
task: "Copa"  # Copa, CB, RTE, BOOLQ, ArcC_Cloze, ArcC_MC

# ========== è®­ç»ƒæ–¹æ³• ==========
method: "zo"  # zo (MeZO), fo_full (Full Fine-tune), fo_lora (LoRA)

# ========== æ•°æ®é…ç½® ==========
data:
  # ç›´æ¥æŒ‡å®šæ•°æ®è·¯å¾„
  # è·¯å¾„æ ¼å¼ï¼šData_v2/synthetic/{æ•°æ®ç”Ÿæˆç›®çš„}/{å®éªŒID}/{Dataset}
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"

  # ğŸ“Œ è¯´æ˜ï¼š
  # - æ•°æ®æ¥è‡ª"prompt_engineering"å®éªŒï¼ˆé˜¶æ®µ1çš„å®éªŒç›®çš„ï¼‰
  # - ä½†è®­ç»ƒç›®çš„æ˜¯"hyperparameter_tuning"ï¼ˆé˜¶æ®µ2çš„å®éªŒç›®çš„ï¼‰
  # - è¿™ä¸¤è€…æ˜¯ç‹¬ç«‹çš„ï¼

# ========== è¶…å‚æ•°é…ç½® ==========
hyperparameters:
  # å­¦ä¹ ç‡ç½‘æ ¼æœç´¢
  learning_rate:
    - 1e-6   # è¾ƒå¤§å­¦ä¹ ç‡
    - 5e-7   # æ¨èå€¼
    - 2e-7   # è¾ƒå°å­¦ä¹ ç‡
    - 1e-7   # æœ€å°å­¦ä¹ ç‡

  batch_size: 16  # Batch size

  steps: 20000  # è®­ç»ƒæ­¥æ•°

  seed: 0  # éšæœºç§å­ï¼ˆå¯ä»¥è®¾ç½®å¤šä¸ªseedè¿›è¡Œé‡å¤å®éªŒï¼‰

  # MeZO ç‰¹å®šå‚æ•°
  zo_eps: 1e-3  # MeZO çš„ epsilon å‚æ•°

# ========== CUDAè®¾å¤‡é…ç½® ==========
cuda_devices: "0"  # ä½¿ç”¨GPU 0ï¼Œå¯ä»¥æ˜¯ "0,1" ä½¿ç”¨å¤šGPU

# ========== è¿è¡Œé€‰é¡¹ ==========
wait_for_completion: false  # åå°è¿è¡Œï¼Œä¸ç­‰å¾…å®Œæˆ
continue_on_error: true  # å‡ºé”™æ—¶ç»§ç»­æ‰§è¡Œå‰©ä½™å®éªŒ

# ========== é¢„æœŸç»“æœç›®å½• ==========
# Results_v2/hyperparameter_tuning/meta-llama/Llama-3.2-1B/
# â”œâ”€â”€ Copa_zo_copa_mezo_v1_1_6/
# â”‚   â””â”€â”€ 20251226_HHMMSS/
# â”‚       â”œâ”€â”€ experiment_config.yaml
# â”‚       â”œâ”€â”€ 1e-06_train.out
# â”‚       â””â”€â”€ 1e-06_train.err
# â”œâ”€â”€ Copa_zo_copa_mezo_v1_5_7/
# â”‚   â””â”€â”€ 20251226_HHMMSS/
# â”œâ”€â”€ Copa_zo_copa_mezo_v1_2_7/
# â”‚   â””â”€â”€ 20251226_HHMMSS/
# â””â”€â”€ Copa_zo_copa_mezo_v1_1_7/
#     â””â”€â”€ 20251226_HHMMSS/

# ========== æ›´å¤šå®éªŒç›®çš„ç¤ºä¾‹ ==========
# è®­ç»ƒå®éªŒç›®çš„ï¼ˆé˜¶æ®µ2ï¼‰æ¨èç±»åˆ«ï¼š
#   - baseline_comparison: ä¸baselineå¯¹æ¯”
#   - model_comparison: å¯¹æ¯”ä¸åŒæ¨¡å‹
#   - hyperparameter_tuning: è¶…å‚æ•°è°ƒä¼˜
#   - ablation_study: æ¶ˆèå®éªŒ
#   - prompt_effectiveness: æµ‹è¯•promptæ•ˆæœ
#   - data_quality_impact: æµ‹è¯•æ•°æ®è´¨é‡å½±å“
#   - scaling_study: æ‰©å±•æ€§ç ”ç©¶

# ========== ä½¿ç”¨æ–¹æ³• ==========
# 1. æ‰§è¡Œè®­ç»ƒï¼š
#    python automation/stage2_training/trainer.py automation/configs/examples/stage2_full_example_hyperparameter_tuning.yaml
#
# 2. é¢„è§ˆå‘½ä»¤ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰ï¼š
#    python automation/stage2_training/trainer.py automation/configs/examples/stage2_full_example_hyperparameter_tuning.yaml --dry-run
#
# 3. æŸ¥çœ‹ç»“æœï¼š
#    python automation/stage2_training/list_results.py
#    python automation/stage2_training/list_results.py --detail --purpose hyperparameter_tuning
