# Complete Stage 2 Training Configuration Example - Hyperparameter tuningExperiment
# Train MeZO using synthetic data generated in Stage 1

# ========== Experiment Management ==========
experiment:
  # ğŸ”´ Important: Stage 1 and Stage 2 experiment purposes are independent!
  #
  # ã€Stage 1 - Data generation experiment purposeã€‘ï¼š
  #   Data_v2/synthetic/prompt_engineering/copa_mezo_v1/
  #   â†‘ purpose = "prompt_engineering"ï¼ˆTest impact of different prompts on data qualityï¼‰
  #
  # ã€Stage 2 - Training experiment purposeã€‘ï¼š
  #   purpose = "hyperparameter_tuning"ï¼ˆTest impact of different learning rates on training performanceï¼‰
  #   â†‘ This value determines where results are saved
  #
  # ã€Results save locationã€‘ï¼š
  #   Results_v2/hyperparameter_tuning/  â† Classified by "training purpose", not data generation purpose!
  #
  # ã€Why separateã€‘ï¼š
  #   - Same dataset may be used for multiple different training experiments
  #   - For example: use prompt_engineering data to test model_comparison
  #   - If not separated, all results will be mixed in prompt_engineering directory
  #
  purpose: "hyperparameter_tuning"  # Training experiment purpose (determines results classification)
  description: "Test effects of different learning rates using copa_mezo_v1 synthetic data"

# ========== Model Configuration ==========
model: "meta-llama/Llama-3.2-1B"

# ========== Task ==========
task: "Copa"  # Copa, CB, RTE, BOOLQ, ArcC_Cloze, ArcC_MC

# ========== Training Method ==========
method: "zo"  # zo (MeZO), fo_full (Full Fine-tune), fo_lora (LoRA)

# ========== Data Configuration ==========
data:
  # Directly specify data path
  # Path format:Data_v2/synthetic/{Data generation purpose}/{Experiment ID}/{Dataset}
  path: "Data_v2/synthetic/prompt_engineering/copa_mezo_v1/Copa"

  # ğŸ“Œ Noteï¼š
  # - Data from"prompt_engineering"experiment (Stage 1 experiment purpose)
  # - But training purpose is"hyperparameter_tuning"(Stage 2 experiment purpose)
  # - These two are independent!

# ========== Hyperparameter Configuration ==========
hyperparameters:
  # Learning Rate Grid Search
  learning_rate:
    - 1e-6   # Larger learning rate
    - 5e-7   # Recommended value
    - 2e-7   # Smaller learning rate
    - 1e-7   # Minimum learning rate

  batch_size: 16  # Batch Size

  steps: 20000  # Training steps

  seed: 0  # Random seed (can set multiple seeds for repeated experiments)

  # MeZO Specific parameters
  zo_eps: 1e-3  # MeZO epsilon parameter

# ========== CUDA Device Configuration ==========
cuda_devices: "0"  # Use GPU 0, can be "0,1" to use multiple GPUs

# ========== Run Options ==========
wait_for_completion: false  # Run in background, do not wait for completion
continue_on_error: true  # Continue with remaining experiments on error

# ========== Expected Results Directory ==========
# Results_v2/hyperparameter_tuning/meta-llama/Llama-3.2-1B/
# â”œâ”€â”€ Copa_zo_copa_mezo_v1_1_6/
# â”‚   â””â”€â”€ 20251226_HHMMSS/
# â”‚       â”œâ”€â”€ experiment_config.yaml
# â”‚       â”œâ”€â”€ 1e-06_train.out
# â”‚       â””â”€â”€ 1e-06_train.err
# â”œâ”€â”€ Copa_zo_copa_mezo_v1_5_7/
# â”‚   â””â”€â”€ 20251226_HHMMSS/
# â”œâ”€â”€ Copa_zo_copa_mezo_v1_2_7/
# â”‚   â””â”€â”€ 20251226_HHMMSS/
# â””â”€â”€ Copa_zo_copa_mezo_v1_1_7/
#     â””â”€â”€ 20251226_HHMMSS/

# ========== More Experiment Purpose Examples ==========
# Training experiment purpose (Stage 2) recommended categories:
#   - baseline_comparison: Comparison with baseline
#   - model_comparison: Compare different models
#   - hyperparameter_tuning: Hyperparameter tuning
#   - ablation_study: Ablation study
#   - prompt_effectiveness: Test prompt effectiveness
#   - data_quality_impact: Test data quality impact
#   - scaling_study: Scaling study

# ========== Usage ==========
# 1. Run training:
#    python automation/stage2_training/trainer.py automation/configs/examples/stage2_full_example_hyperparameter_tuning.yaml
#
# 2. Preview command (without actually executing):
#    python automation/stage2_training/trainer.py automation/configs/examples/stage2_full_example_hyperparameter_tuning.yaml --dry-run
#
# 3. viewResultï¼š
#    python automation/stage2_training/list_results.py
#    python automation/stage2_training/list_results.py --detail --purpose hyperparameter_tuning
